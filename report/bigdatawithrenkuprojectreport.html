<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_au1x8f4nspcp-8{list-style-type:none}ul.lst-kix_au1x8f4nspcp-5{list-style-type:none}ul.lst-kix_au1x8f4nspcp-4{list-style-type:none}ul.lst-kix_au1x8f4nspcp-7{list-style-type:none}ul.lst-kix_au1x8f4nspcp-6{list-style-type:none}.lst-kix_7pa4viqc9fdm-8>li:before{content:"\0025a0  "}.lst-kix_7pa4viqc9fdm-7>li:before{content:"\0025cb  "}.lst-kix_7pa4viqc9fdm-6>li:before{content:"\0025cf  "}.lst-kix_7pa4viqc9fdm-4>li:before{content:"\0025cb  "}.lst-kix_7pa4viqc9fdm-3>li:before{content:"\0025cf  "}.lst-kix_7pa4viqc9fdm-5>li:before{content:"\0025a0  "}.lst-kix_xlcvmb2pgsb9-7>li:before{content:"\0025cb  "}.lst-kix_xlcvmb2pgsb9-8>li:before{content:"\0025a0  "}ul.lst-kix_au1x8f4nspcp-1{list-style-type:none}ul.lst-kix_au1x8f4nspcp-0{list-style-type:none}ul.lst-kix_au1x8f4nspcp-3{list-style-type:none}.lst-kix_7pa4viqc9fdm-1>li:before{content:"\0025cb  "}ul.lst-kix_au1x8f4nspcp-2{list-style-type:none}.lst-kix_7pa4viqc9fdm-2>li:before{content:"\0025a0  "}.lst-kix_xlcvmb2pgsb9-1>li:before{content:"\0025cb  "}.lst-kix_xlcvmb2pgsb9-3>li:before{content:"\0025cf  "}.lst-kix_59rc55pj62nw-8>li:before{content:"\0025a0  "}.lst-kix_xlcvmb2pgsb9-0>li:before{content:"\0025cf  "}.lst-kix_xlcvmb2pgsb9-4>li:before{content:"\0025cb  "}.lst-kix_hfkctp93zj4-0>li:before{content:"\0025cf  "}.lst-kix_xlcvmb2pgsb9-5>li:before{content:"\0025a0  "}.lst-kix_59rc55pj62nw-6>li:before{content:"\0025cf  "}.lst-kix_hfkctp93zj4-1>li:before{content:"\0025cb  "}.lst-kix_xlcvmb2pgsb9-6>li:before{content:"\0025cf  "}.lst-kix_59rc55pj62nw-7>li:before{content:"\0025cb  "}.lst-kix_hfkctp93zj4-4>li:before{content:"\0025cb  "}.lst-kix_hfkctp93zj4-2>li:before{content:"\0025a0  "}.lst-kix_hfkctp93zj4-6>li:before{content:"\0025cf  "}.lst-kix_hfkctp93zj4-3>li:before{content:"\0025cf  "}.lst-kix_hfkctp93zj4-7>li:before{content:"\0025cb  "}.lst-kix_hfkctp93zj4-5>li:before{content:"\0025a0  "}.lst-kix_xlcvmb2pgsb9-2>li:before{content:"\0025a0  "}.lst-kix_40wnwpr51y2c-2>li:before{content:"\0025a0  "}ul.lst-kix_676b581t2bm5-8{list-style-type:none}ul.lst-kix_676b581t2bm5-7{list-style-type:none}.lst-kix_40wnwpr51y2c-3>li:before{content:"\0025cf  "}ul.lst-kix_676b581t2bm5-6{list-style-type:none}.lst-kix_40wnwpr51y2c-0>li:before{content:"\0025cf  "}.lst-kix_40wnwpr51y2c-1>li:before{content:"\0025cb  "}.lst-kix_59rc55pj62nw-2>li:before{content:"\0025a0  "}.lst-kix_59rc55pj62nw-0>li:before{content:"\0025cf  "}.lst-kix_59rc55pj62nw-4>li:before{content:"\0025cb  "}.lst-kix_59rc55pj62nw-1>li:before{content:"\0025cb  "}.lst-kix_59rc55pj62nw-5>li:before{content:"\0025a0  "}ul.lst-kix_676b581t2bm5-5{list-style-type:none}ul.lst-kix_676b581t2bm5-4{list-style-type:none}ul.lst-kix_676b581t2bm5-3{list-style-type:none}ul.lst-kix_676b581t2bm5-2{list-style-type:none}ul.lst-kix_676b581t2bm5-1{list-style-type:none}ul.lst-kix_676b581t2bm5-0{list-style-type:none}.lst-kix_59rc55pj62nw-3>li:before{content:"\0025cf  "}.lst-kix_6gjo17l6cnof-8>li:before{content:"\0025a0  "}ul.lst-kix_5ae22pxt9720-6{list-style-type:none}ul.lst-kix_5ae22pxt9720-7{list-style-type:none}ul.lst-kix_5ae22pxt9720-8{list-style-type:none}ul.lst-kix_5ae22pxt9720-2{list-style-type:none}ul.lst-kix_5ae22pxt9720-3{list-style-type:none}ul.lst-kix_5ae22pxt9720-4{list-style-type:none}ul.lst-kix_5ae22pxt9720-5{list-style-type:none}ul.lst-kix_6gjo17l6cnof-5{list-style-type:none}ul.lst-kix_6gjo17l6cnof-6{list-style-type:none}ul.lst-kix_tjsytd3xsrk2-1{list-style-type:none}ul.lst-kix_5ae22pxt9720-0{list-style-type:none}ul.lst-kix_6gjo17l6cnof-7{list-style-type:none}ul.lst-kix_tjsytd3xsrk2-0{list-style-type:none}ul.lst-kix_5ae22pxt9720-1{list-style-type:none}ul.lst-kix_6gjo17l6cnof-8{list-style-type:none}ul.lst-kix_6gjo17l6cnof-1{list-style-type:none}ul.lst-kix_6gjo17l6cnof-2{list-style-type:none}ul.lst-kix_6gjo17l6cnof-3{list-style-type:none}ul.lst-kix_6gjo17l6cnof-4{list-style-type:none}ul.lst-kix_tjsytd3xsrk2-7{list-style-type:none}ul.lst-kix_tjsytd3xsrk2-6{list-style-type:none}ul.lst-kix_tjsytd3xsrk2-8{list-style-type:none}ul.lst-kix_6gjo17l6cnof-0{list-style-type:none}ul.lst-kix_tjsytd3xsrk2-3{list-style-type:none}ul.lst-kix_tjsytd3xsrk2-2{list-style-type:none}ul.lst-kix_tjsytd3xsrk2-5{list-style-type:none}ul.lst-kix_tjsytd3xsrk2-4{list-style-type:none}ul.lst-kix_qtthvhbbkxv3-1{list-style-type:none}ul.lst-kix_qtthvhbbkxv3-0{list-style-type:none}ul.lst-kix_qtthvhbbkxv3-3{list-style-type:none}ul.lst-kix_qtthvhbbkxv3-2{list-style-type:none}ul.lst-kix_qtthvhbbkxv3-5{list-style-type:none}.lst-kix_au1x8f4nspcp-3>li:before{content:"\0025cf  "}ul.lst-kix_qtthvhbbkxv3-4{list-style-type:none}.lst-kix_tjsytd3xsrk2-8>li:before{content:"\0025a0  "}ul.lst-kix_qtthvhbbkxv3-7{list-style-type:none}ul.lst-kix_qtthvhbbkxv3-6{list-style-type:none}ul.lst-kix_qtthvhbbkxv3-8{list-style-type:none}.lst-kix_tjsytd3xsrk2-6>li:before{content:"\0025cf  "}.lst-kix_676b581t2bm5-4>li:before{content:"\0025cb  "}ul.lst-kix_hfkctp93zj4-0{list-style-type:none}.lst-kix_au1x8f4nspcp-5>li:before{content:"\0025a0  "}.lst-kix_676b581t2bm5-2>li:before{content:"\0025a0  "}ul.lst-kix_d0wpst8r9ad4-1{list-style-type:none}ul.lst-kix_d0wpst8r9ad4-2{list-style-type:none}ul.lst-kix_d0wpst8r9ad4-3{list-style-type:none}ul.lst-kix_d0wpst8r9ad4-4{list-style-type:none}ul.lst-kix_d0wpst8r9ad4-5{list-style-type:none}ul.lst-kix_236yswgy0fd1-2{list-style-type:none}ul.lst-kix_d0wpst8r9ad4-6{list-style-type:none}ul.lst-kix_236yswgy0fd1-3{list-style-type:none}ul.lst-kix_d0wpst8r9ad4-7{list-style-type:none}ul.lst-kix_236yswgy0fd1-0{list-style-type:none}.lst-kix_au1x8f4nspcp-7>li:before{content:"\0025cb  "}ul.lst-kix_d0wpst8r9ad4-8{list-style-type:none}ul.lst-kix_236yswgy0fd1-1{list-style-type:none}.lst-kix_676b581t2bm5-0>li:before{content:"\0025cf  "}ul.lst-kix_d0wpst8r9ad4-0{list-style-type:none}.lst-kix_6gjo17l6cnof-7>li:before{content:"\0025cb  "}.lst-kix_d0wpst8r9ad4-1>li:before{content:"\0025cb  "}.lst-kix_6gjo17l6cnof-5>li:before{content:"\0025a0  "}.lst-kix_tjsytd3xsrk2-0>li:before{content:"\0025cf  "}.lst-kix_6gjo17l6cnof-3>li:before{content:"\0025cf  "}.lst-kix_qtthvhbbkxv3-8>li:before{content:"\0025a0  "}.lst-kix_tjsytd3xsrk2-4>li:before{content:"\0025cb  "}.lst-kix_d0wpst8r9ad4-7>li:before{content:"\0025cb  "}.lst-kix_5ae22pxt9720-0>li:before{content:"\0025cf  "}.lst-kix_d0wpst8r9ad4-5>li:before{content:"\0025a0  "}.lst-kix_6gjo17l6cnof-1>li:before{content:"\0025cb  "}.lst-kix_40wnwpr51y2c-7>li:before{content:"\0025cb  "}.lst-kix_d0wpst8r9ad4-3>li:before{content:"\0025cf  "}.lst-kix_tjsytd3xsrk2-2>li:before{content:"\0025a0  "}.lst-kix_40wnwpr51y2c-5>li:before{content:"\0025a0  "}.lst-kix_hfkctp93zj4-8>li:before{content:"\0025a0  "}.lst-kix_5ae22pxt9720-2>li:before{content:"\0025a0  "}.lst-kix_8cgkwypva7r0-4>li:before{content:"\0025cb  "}.lst-kix_2s2l0b9jwmtb-7>li:before{content:"\0025cb  "}.lst-kix_5ae22pxt9720-4>li:before{content:"\0025cb  "}.lst-kix_5ae22pxt9720-6>li:before{content:"\0025cf  "}.lst-kix_8cgkwypva7r0-2>li:before{content:"\0025a0  "}.lst-kix_ocz5h2tob8yz-3>li:before{content:"\0025cf  "}.lst-kix_2s2l0b9jwmtb-1>li:before{content:"\0025cb  "}.lst-kix_2s2l0b9jwmtb-3>li:before{content:"\0025cf  "}.lst-kix_5ae22pxt9720-8>li:before{content:"\0025a0  "}.lst-kix_236yswgy0fd1-1>li:before{content:"\0025cb  "}.lst-kix_236yswgy0fd1-3>li:before{content:"\0025cf  "}.lst-kix_ocz5h2tob8yz-1>li:before{content:"\0025cb  "}.lst-kix_7pa4viqc9fdm-0>li:before{content:"\0025cf  "}ul.lst-kix_8cgkwypva7r0-0{list-style-type:none}.lst-kix_2s2l0b9jwmtb-5>li:before{content:"\0025a0  "}ul.lst-kix_8cgkwypva7r0-1{list-style-type:none}.lst-kix_236yswgy0fd1-5>li:before{content:"\0025a0  "}.lst-kix_236yswgy0fd1-7>li:before{content:"\0025cb  "}ul.lst-kix_8cgkwypva7r0-2{list-style-type:none}.lst-kix_8cgkwypva7r0-6>li:before{content:"\0025cf  "}ul.lst-kix_8cgkwypva7r0-3{list-style-type:none}ul.lst-kix_8cgkwypva7r0-4{list-style-type:none}ul.lst-kix_8cgkwypva7r0-5{list-style-type:none}ul.lst-kix_8cgkwypva7r0-6{list-style-type:none}ul.lst-kix_2s2l0b9jwmtb-0{list-style-type:none}ul.lst-kix_8cgkwypva7r0-7{list-style-type:none}ul.lst-kix_8cgkwypva7r0-8{list-style-type:none}ul.lst-kix_2s2l0b9jwmtb-2{list-style-type:none}ul.lst-kix_2s2l0b9jwmtb-1{list-style-type:none}ul.lst-kix_2s2l0b9jwmtb-4{list-style-type:none}ul.lst-kix_2s2l0b9jwmtb-3{list-style-type:none}ul.lst-kix_2s2l0b9jwmtb-6{list-style-type:none}.lst-kix_8cgkwypva7r0-8>li:before{content:"\0025a0  "}ul.lst-kix_2s2l0b9jwmtb-5{list-style-type:none}.lst-kix_676b581t2bm5-6>li:before{content:"\0025cf  "}ul.lst-kix_2s2l0b9jwmtb-8{list-style-type:none}ul.lst-kix_2s2l0b9jwmtb-7{list-style-type:none}.lst-kix_676b581t2bm5-8>li:before{content:"\0025a0  "}.lst-kix_d05pnh2rmjj9-1>li:before{content:"\0025cb  "}.lst-kix_au1x8f4nspcp-1>li:before{content:"\0025cb  "}ul.lst-kix_ocz5h2tob8yz-6{list-style-type:none}ul.lst-kix_ocz5h2tob8yz-7{list-style-type:none}ul.lst-kix_ocz5h2tob8yz-8{list-style-type:none}.lst-kix_d05pnh2rmjj9-0>li:before{content:"\0025cf  "}.lst-kix_d05pnh2rmjj9-3>li:before{content:"\0025cf  "}.lst-kix_d05pnh2rmjj9-4>li:before{content:"\0025cb  "}.lst-kix_d05pnh2rmjj9-5>li:before{content:"\0025a0  "}ul.lst-kix_xlcvmb2pgsb9-0{list-style-type:none}.lst-kix_d05pnh2rmjj9-7>li:before{content:"\0025cb  "}ul.lst-kix_xlcvmb2pgsb9-1{list-style-type:none}ul.lst-kix_xlcvmb2pgsb9-2{list-style-type:none}.lst-kix_d05pnh2rmjj9-6>li:before{content:"\0025cf  "}.lst-kix_d05pnh2rmjj9-8>li:before{content:"\0025a0  "}ul.lst-kix_xlcvmb2pgsb9-3{list-style-type:none}.lst-kix_236yswgy0fd1-8>li:before{content:"\0025a0  "}ul.lst-kix_ajurcepwj74v-0{list-style-type:none}ul.lst-kix_ocz5h2tob8yz-0{list-style-type:none}ul.lst-kix_ajurcepwj74v-1{list-style-type:none}ul.lst-kix_ocz5h2tob8yz-1{list-style-type:none}ul.lst-kix_ajurcepwj74v-2{list-style-type:none}ul.lst-kix_ocz5h2tob8yz-2{list-style-type:none}ul.lst-kix_ocz5h2tob8yz-3{list-style-type:none}ul.lst-kix_ocz5h2tob8yz-4{list-style-type:none}ul.lst-kix_ocz5h2tob8yz-5{list-style-type:none}.lst-kix_8cgkwypva7r0-0>li:before{content:"\0025cf  "}ul.lst-kix_ajurcepwj74v-7{list-style-type:none}ul.lst-kix_ajurcepwj74v-8{list-style-type:none}.lst-kix_8cgkwypva7r0-1>li:before{content:"\0025cb  "}ul.lst-kix_ajurcepwj74v-3{list-style-type:none}.lst-kix_ocz5h2tob8yz-5>li:before{content:"\0025a0  "}ul.lst-kix_ajurcepwj74v-4{list-style-type:none}ul.lst-kix_ajurcepwj74v-5{list-style-type:none}ul.lst-kix_ajurcepwj74v-6{list-style-type:none}ul.lst-kix_xlcvmb2pgsb9-8{list-style-type:none}.lst-kix_ocz5h2tob8yz-6>li:before{content:"\0025cf  "}.lst-kix_ocz5h2tob8yz-7>li:before{content:"\0025cb  "}ul.lst-kix_xlcvmb2pgsb9-4{list-style-type:none}ul.lst-kix_xlcvmb2pgsb9-5{list-style-type:none}.lst-kix_ocz5h2tob8yz-8>li:before{content:"\0025a0  "}ul.lst-kix_xlcvmb2pgsb9-6{list-style-type:none}ul.lst-kix_xlcvmb2pgsb9-7{list-style-type:none}.lst-kix_ajurcepwj74v-8>li:before{content:"\0025a0  "}.lst-kix_ajurcepwj74v-6>li:before{content:"\0025cf  "}.lst-kix_ajurcepwj74v-7>li:before{content:"\0025cb  "}.lst-kix_qtthvhbbkxv3-0>li:before{content:"\0025cf  "}.lst-kix_qtthvhbbkxv3-1>li:before{content:"\0025cb  "}.lst-kix_ajurcepwj74v-1>li:before{content:"\0025cb  "}.lst-kix_qtthvhbbkxv3-4>li:before{content:"\0025cb  "}.lst-kix_qtthvhbbkxv3-5>li:before{content:"\0025a0  "}.lst-kix_ajurcepwj74v-2>li:before{content:"\0025a0  "}.lst-kix_qtthvhbbkxv3-2>li:before{content:"\0025a0  "}.lst-kix_qtthvhbbkxv3-3>li:before{content:"\0025cf  "}.lst-kix_qtthvhbbkxv3-6>li:before{content:"\0025cf  "}.lst-kix_qtthvhbbkxv3-7>li:before{content:"\0025cb  "}.lst-kix_ajurcepwj74v-3>li:before{content:"\0025cf  "}.lst-kix_ajurcepwj74v-4>li:before{content:"\0025cb  "}.lst-kix_ajurcepwj74v-5>li:before{content:"\0025a0  "}ul.lst-kix_236yswgy0fd1-6{list-style-type:none}ul.lst-kix_236yswgy0fd1-7{list-style-type:none}ul.lst-kix_236yswgy0fd1-4{list-style-type:none}ul.lst-kix_236yswgy0fd1-5{list-style-type:none}ul.lst-kix_236yswgy0fd1-8{list-style-type:none}ul.lst-kix_hfkctp93zj4-3{list-style-type:none}.lst-kix_ajurcepwj74v-0>li:before{content:"\0025cf  "}ul.lst-kix_hfkctp93zj4-4{list-style-type:none}ul.lst-kix_hfkctp93zj4-1{list-style-type:none}ul.lst-kix_hfkctp93zj4-2{list-style-type:none}ul.lst-kix_hfkctp93zj4-7{list-style-type:none}ul.lst-kix_hfkctp93zj4-8{list-style-type:none}ul.lst-kix_hfkctp93zj4-5{list-style-type:none}ul.lst-kix_hfkctp93zj4-6{list-style-type:none}ul.lst-kix_d05pnh2rmjj9-1{list-style-type:none}ul.lst-kix_d05pnh2rmjj9-0{list-style-type:none}ul.lst-kix_d05pnh2rmjj9-3{list-style-type:none}.lst-kix_au1x8f4nspcp-0>li:before{content:"\0025cf  "}ul.lst-kix_d05pnh2rmjj9-2{list-style-type:none}.lst-kix_tjsytd3xsrk2-7>li:before{content:"\0025cb  "}.lst-kix_au1x8f4nspcp-4>li:before{content:"\0025cb  "}.lst-kix_676b581t2bm5-3>li:before{content:"\0025cf  "}.lst-kix_au1x8f4nspcp-6>li:before{content:"\0025cf  "}.lst-kix_au1x8f4nspcp-8>li:before{content:"\0025a0  "}.lst-kix_676b581t2bm5-1>li:before{content:"\0025cb  "}ul.lst-kix_d05pnh2rmjj9-8{list-style-type:none}ul.lst-kix_d05pnh2rmjj9-5{list-style-type:none}ul.lst-kix_d05pnh2rmjj9-4{list-style-type:none}ul.lst-kix_d05pnh2rmjj9-7{list-style-type:none}ul.lst-kix_d05pnh2rmjj9-6{list-style-type:none}ul.lst-kix_59rc55pj62nw-2{list-style-type:none}ul.lst-kix_7pa4viqc9fdm-1{list-style-type:none}ul.lst-kix_59rc55pj62nw-1{list-style-type:none}ul.lst-kix_7pa4viqc9fdm-0{list-style-type:none}.lst-kix_d0wpst8r9ad4-0>li:before{content:"\0025cf  "}ul.lst-kix_59rc55pj62nw-4{list-style-type:none}.lst-kix_6gjo17l6cnof-6>li:before{content:"\0025cf  "}ul.lst-kix_59rc55pj62nw-3{list-style-type:none}ul.lst-kix_59rc55pj62nw-0{list-style-type:none}ul.lst-kix_7pa4viqc9fdm-8{list-style-type:none}.lst-kix_6gjo17l6cnof-2>li:before{content:"\0025a0  "}.lst-kix_6gjo17l6cnof-4>li:before{content:"\0025cb  "}ul.lst-kix_7pa4viqc9fdm-7{list-style-type:none}ul.lst-kix_7pa4viqc9fdm-6{list-style-type:none}ul.lst-kix_59rc55pj62nw-6{list-style-type:none}ul.lst-kix_7pa4viqc9fdm-5{list-style-type:none}ul.lst-kix_59rc55pj62nw-5{list-style-type:none}ul.lst-kix_7pa4viqc9fdm-4{list-style-type:none}ul.lst-kix_59rc55pj62nw-8{list-style-type:none}ul.lst-kix_7pa4viqc9fdm-3{list-style-type:none}ul.lst-kix_59rc55pj62nw-7{list-style-type:none}ul.lst-kix_7pa4viqc9fdm-2{list-style-type:none}.lst-kix_tjsytd3xsrk2-3>li:before{content:"\0025cf  "}.lst-kix_tjsytd3xsrk2-5>li:before{content:"\0025a0  "}.lst-kix_d0wpst8r9ad4-6>li:before{content:"\0025cf  "}.lst-kix_6gjo17l6cnof-0>li:before{content:"\0025cf  "}.lst-kix_40wnwpr51y2c-6>li:before{content:"\0025cf  "}.lst-kix_tjsytd3xsrk2-1>li:before{content:"\0025cb  "}.lst-kix_d0wpst8r9ad4-2>li:before{content:"\0025a0  "}.lst-kix_d0wpst8r9ad4-4>li:before{content:"\0025cb  "}.lst-kix_40wnwpr51y2c-4>li:before{content:"\0025cb  "}.lst-kix_40wnwpr51y2c-8>li:before{content:"\0025a0  "}.lst-kix_5ae22pxt9720-1>li:before{content:"\0025cb  "}.lst-kix_d0wpst8r9ad4-8>li:before{content:"\0025a0  "}.lst-kix_5ae22pxt9720-3>li:before{content:"\0025cf  "}.lst-kix_2s2l0b9jwmtb-6>li:before{content:"\0025cf  "}.lst-kix_8cgkwypva7r0-3>li:before{content:"\0025cf  "}.lst-kix_2s2l0b9jwmtb-8>li:before{content:"\0025a0  "}.lst-kix_ocz5h2tob8yz-4>li:before{content:"\0025cb  "}.lst-kix_5ae22pxt9720-5>li:before{content:"\0025a0  "}.lst-kix_2s2l0b9jwmtb-2>li:before{content:"\0025a0  "}.lst-kix_ocz5h2tob8yz-2>li:before{content:"\0025a0  "}.lst-kix_5ae22pxt9720-7>li:before{content:"\0025cb  "}.lst-kix_236yswgy0fd1-0>li:before{content:"\0025cf  "}.lst-kix_236yswgy0fd1-4>li:before{content:"\0025cb  "}.lst-kix_8cgkwypva7r0-5>li:before{content:"\0025a0  "}.lst-kix_2s2l0b9jwmtb-4>li:before{content:"\0025cb  "}.lst-kix_ocz5h2tob8yz-0>li:before{content:"\0025cf  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_236yswgy0fd1-6>li:before{content:"\0025cf  "}ul.lst-kix_40wnwpr51y2c-3{list-style-type:none}ul.lst-kix_40wnwpr51y2c-4{list-style-type:none}ul.lst-kix_40wnwpr51y2c-1{list-style-type:none}.lst-kix_8cgkwypva7r0-7>li:before{content:"\0025cb  "}ul.lst-kix_40wnwpr51y2c-2{list-style-type:none}ul.lst-kix_40wnwpr51y2c-7{list-style-type:none}ul.lst-kix_40wnwpr51y2c-8{list-style-type:none}.lst-kix_676b581t2bm5-5>li:before{content:"\0025a0  "}ul.lst-kix_40wnwpr51y2c-5{list-style-type:none}ul.lst-kix_40wnwpr51y2c-6{list-style-type:none}.lst-kix_d05pnh2rmjj9-2>li:before{content:"\0025a0  "}.lst-kix_au1x8f4nspcp-2>li:before{content:"\0025a0  "}.lst-kix_2s2l0b9jwmtb-0>li:before{content:"\0025cf  "}ul.lst-kix_40wnwpr51y2c-0{list-style-type:none}.lst-kix_676b581t2bm5-7>li:before{content:"\0025cb  "}.lst-kix_236yswgy0fd1-2>li:before{content:"\0025a0  "}ol{margin:0;padding:0}table td,table th{padding:0}.c11{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#f3f3f3;border-left-style:solid;border-bottom-width:1pt;width:451.4pt;border-top-color:#000000;border-bottom-style:solid}.c16{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left;height:20pt}.c21{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c19{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c18{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c32{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Courier New";font-style:normal}.c10{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c17{margin-left:36pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c5{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c25{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:24pt;font-family:"Arial";font-style:normal}.c24{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c29{padding-top:4pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c39{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c38{padding-top:10pt;padding-bottom:4pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c27{color:#000000;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Arial";font-style:normal}.c20{padding-top:10pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c8{background-color:#efefef;font-size:10pt;font-family:"Courier New";font-weight:400}.c28{border-spacing:0;border-collapse:collapse;margin-right:auto}.c9{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c26{font-size:9pt;font-family:"Courier New";font-weight:400}.c36{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c6{color:inherit;text-decoration:inherit}.c15{margin-left:36pt;padding-left:0pt}.c33{width:33%;height:1px}.c30{padding:0;margin:0}.c7{font-weight:700}.c37{height:12.8pt}.c22{font-style:italic}.c1{height:11pt}.c35{height:0pt}.c12{font-size:10pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c36"><div><p class="c0 c1"><span class="c3"></span></p></div><p class="c14 c1"><span class="c3"></span></p><p class="c14"><span>Semester Project Report </span><span>COM-412</span></p><p class="c14 c1"><span class="c3"></span></p><p class="c14"><span class="c3">Spring 2021</span></p><p class="c14 c1"><span class="c3"></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c1 c18 title" id="h.zfh2142hnt6y"><span class="c34"></span></p><p class="c18 title" id="h.bj9uhwt7ojfe"><span class="c34">Systems Integration in Renku for Big Data computing with the Hadoop Framework</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c14"><span class="c3">Student: Jules Gottraux</span></p><p class="c14 c1"><span class="c3"></span></p><p class="c14"><span class="c3">Supervisors: Eric Bouillet and Sofiane Sarni</span></p><p class="c14 c1"><span class="c3"></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c29"><span class="c5"><a class="c6" href="#h.3jnbnjnrl1qb">Introduction</a></span><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5"><a class="c6" href="#h.3jnbnjnrl1qb">0</a></span></p><p class="c20"><span class="c5"><a class="c6" href="#h.83uhzmhuxv36">Motivation and Objectives</a></span><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5"><a class="c6" href="#h.83uhzmhuxv36">1</a></span></p><p class="c20"><span class="c5"><a class="c6" href="#h.t8oalnhnv6d8">Background</a></span><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5"><a class="c6" href="#h.t8oalnhnv6d8">1</a></span></p><p class="c10"><span class="c3"><a class="c6" href="#h.78440tjzttuq">Overview of Hadoop</a></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3"><a class="c6" href="#h.78440tjzttuq">1</a></span></p><p class="c10"><span class="c5"><a class="c6" href="#h.zfpc7ihw5t4l">Overview of Renku</a></span><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5"><a class="c6" href="#h.zfpc7ihw5t4l">1</a></span></p><p class="c10"><span class="c5"><a class="c6" href="#h.c2ycqdp4svb">Integration of Renku with Hadoop - Current Situation</a></span><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5"><a class="c6" href="#h.c2ycqdp4svb">2</a></span></p><p class="c20"><span class="c5"><a class="c6" href="#h.r9076n55opcc">Description of the Work</a></span><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5"><a class="c6" href="#h.r9076n55opcc">3</a></span></p><p class="c10"><span class="c3"><a class="c6" href="#h.smijrirmrngb">Preparation of the environments</a></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3"><a class="c6" href="#h.smijrirmrngb">3</a></span></p><p class="c17"><span class="c3"><a class="c6" href="#h.e57lmohqmut4">Renku deployment</a></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3"><a class="c6" href="#h.e57lmohqmut4">3</a></span></p><p class="c17"><span class="c3"><a class="c6" href="#h.4ftsskc88lff">Hadoop deployment</a></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3"><a class="c6" href="#h.4ftsskc88lff">4</a></span></p><p class="c10"><span class="c5"><a class="c6" href="#h.dwzf3gsibeb9">Hadoop setup in Renku</a></span><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5"><a class="c6" href="#h.dwzf3gsibeb9">5</a></span></p><p class="c17"><span class="c3"><a class="c6" href="#h.fab7opk349hp">Design</a></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3"><a class="c6" href="#h.fab7opk349hp">5</a></span></p><p class="c17"><span class="c3"><a class="c6" href="#h.5hu4v9wss5xa">Issues and possible improvements:</a></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3"><a class="c6" href="#h.5hu4v9wss5xa">8</a></span></p><p class="c17"><span class="c5"><a class="c6" href="#h.8xl93iqhiej1">Authentication to Hadoop services</a></span><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5"><a class="c6" href="#h.8xl93iqhiej1">9</a></span></p><p class="c20"><span class="c7"><a class="c6" href="#h.jo1qk0s25skm">Results</a></span><span class="c7">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c7"><a class="c6" href="#h.jo1qk0s25skm">10</a></span></p><p class="c20"><span class="c5"><a class="c6" href="#h.lvwwp5wa994i">Conclusion and Future Steps</a></span><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5"><a class="c6" href="#h.lvwwp5wa994i">12</a></span></p><p class="c38"><span class="c5"><a class="c6" href="#h.4799g2mp0m22">References</a></span><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5"><a class="c6" href="#h.4799g2mp0m22">12</a></span></p><hr style="page-break-before:always;display:none;"><h1 class="c16" id="h.3jnbnjnrl1qb"><span class="c13"></span></h1><h1 class="c21" id="h.suy05ck8si1w"><span>Introduction</span></h1><p class="c0"><span>The technology provided by </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://www.apache.org/&amp;sa=D&amp;source=editors&amp;ust=1624552266905000&amp;usg=AOvVaw083ezw1olKAj2qyzsEAnP2">Apache</a></span><span>&nbsp;with its Hadoop framework is the foundation of many leading services when it comes to Big Data processing. With this project, we work on improving the accessibility of this framework by providing documentation for all key components and a proposal of design for integrating an automatic and dynamic configuration of Hadoop in </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://datascience.ch/renku/&amp;sa=D&amp;source=editors&amp;ust=1624552266905000&amp;usg=AOvVaw3vDsDwIXtLPebNzgv08CwK">Renku</a></span><span>, an open-source platform for reproducible and collaborative Data Science. Developed by the Swiss Data Science Center (</span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://datascience.ch/&amp;sa=D&amp;source=editors&amp;ust=1624552266906000&amp;usg=AOvVaw0efri5lUwFfNx9ZpgMSbYC">SDSC</a></span><span>), it enables versioning of the code, the data and the environment which together provides full reproducibility of Data Science projects. In the long term by offering a service similar to what we&rsquo;re designing, Renku would be one of the &nbsp;rare providers of an easy-to-use Hadoop environment and it would greatly improve the accessibility to it.</span></p><h1 class="c21" id="h.83uhzmhuxv36"><span class="c13">Motivation and Objectives</span></h1><p class="c0"><span>Services using the Hadoop framework are very powerful, they allow efficient processing on very large datasets thanks to their distributed computation. To work with datasets this big (few Gigabytes to Terabytes), even the most powerful computers reach their limit. Using this framework, computation can scale for massive datasets as it can operate using hundreds or thousands of nodes. This productivity comes with a price, distributed computation based on Hadoop can&rsquo;t be run locally by design and setting up a cluster and a client together is a complicated task. Hence, working with </span><span class="c3">this framework requires access to a configured cluster. The access to such clusters is not easily accessible to the public, and only proprietary services like Amazon provides them.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span class="c3">To increase the accessibility to this framework, we want to add the possibility to easily create Hadoop projects in Renku. By doing so, in the long term we could allow users to gain access to Hadoop clusters and easily work using this tool. This would promote Big Data processing with a free and open-source application.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Thus our objectives are to automate the configuration of Hadoop projects in Renku, freeing the Data Scientists and Analysts of this work, and document all the process for future research including the deployment of the needed components.</span></p><hr style="page-break-before:always;display:none;"><h1 class="c16" id="h.jfba9p1cjioo"><span class="c13"></span></h1><h1 class="c21" id="h.yiiwnf5iepsi"><span class="c13">Background</span></h1><h2 class="c24" id="h.78440tjzttuq"><span class="c31">Overview of Hadoop</span></h2><p class="c0"><span class="c3">Apache Hadoop is a framework for distributed processing. It is composed of the Hadoop File System (HDFS), YARN and a MapReduce component. HDFS is the distributed file system made for Hadoop. YARN is the job scheduler and cluster resource management, it orchestrates the computation across the nodes and manages the resources needed for the computation. MapReduce is the programming model used by Hadoop, it has been designed for parallel and distributed processing on clusters.</span></p><p class="c0"><span class="c3">&nbsp;</span></p><p class="c0"><span>Hadoop serves as the basis for a lot of services for Big Data based on distributed architecture: Spark, Hive, HBase, Kafka, etc. If you are unfamiliar with Hadoop and want to know more, Craig Stedman has made </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://searchdatamanagement.techtarget.com/definition/Hadoop&amp;sa=D&amp;source=editors&amp;ust=1624552266909000&amp;usg=AOvVaw1KwPrO0il_r0y-7CfFCOvp">a great and accessible description of it</a></span><span>. It is a very important tool, it is considered as the principal technology for Big Data processing for more than 10 years (</span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://www.lebigdata.fr/hadoop&amp;sa=D&amp;source=editors&amp;ust=1624552266909000&amp;usg=AOvVaw2n7y-puvm9kTv3RfhUy4Ks">source</a></span><span class="c3">), &nbsp;especially in combination with Spark which is widely used.</span></p><h2 class="c24" id="h.zfpc7ihw5t4l"><span class="c31">Overview of Renku</span></h2><p class="c0"><span class="c3">Renku is a platform for Data Scientists and Analysts, it helps them create reproducible environments for their project. As Git is for the code, Renku enables version control for the code, the data and the computer environment which makes it the perfect tool for collaborative and reproducible Data Science projects. A Renku project is similar to any other project, it just needs some additional information for the creation of the environment, mainly files needed by Docker. Renku is always linked to a Gitlab instance where Renku stores its projects.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span class="c3">Renku is very intuitive to use. From the web UI of Renku a user can choose a project and launch an interactive environment with a command-line interface and the ability to launch jupyter notebooks to code directly in the browser.</span></p><h2 class="c24" id="h.c2ycqdp4svb"><span class="c31">Integration of Renku with Hadoop - Current Situation</span></h2><p class="c0"><span>There are templates available in Renku to set up a new project rapidly. A Renku template sets up a simple environment for the needed task. For example, the template for python will install the packages needed by python along with additional </span><span class="c22">nice-to-have</span><span>&nbsp;packages</span><span class="c3">, add the extension manager to jupyter and improve the command-line prompt with powershell.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span class="c3">For a project using Hadoop, the problem is that there is no integrated support yet. Meaning that to work on a project using Hadoop in Renku, all the configuration has to be done manually by the user. This includes installing the softwares for all services, configuring them and connecting them to their back-end (on the Hadoop cluster). This is problematic for a number of reasons. First, this is messy as all the configuration has to be done directly in the Dockerfile of the project. Hence, there is no separation between the project&rsquo;s code and its setup. This means that not only all people working on the project have to use the same back-end but also that a person working with multiple Hadoop projects has to duplicate the configuration across all of them. This also comes with a severe security flaw, by sharing the project the user shares also the resources he or she used in the project. Even if the cluster is secure, revealing the addresses of the cluster publicly is never a good idea as it exposes the cluster to attacks (e.g. DDoS). Secondly, installing and setting up the local machine for Hadoop with the back-end is not a trivial task. Most of the work will be common to all projects and thus easily automated. But done by hand this process takes time and involves a lot of transversal knowledge. In fact, chances are that without explicit support for Hadoop in Renku, hardly anybody will take the effort to use Renku with Hadoop. And finally, installing all the softwares in the Dockerfile will make the creation of the environment significantly slower than if it was already installed in the Docker image.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>To illustrate this current state in Renku, we&rsquo;ve added </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://github.com/jjjules/renku-hadoop/tree/master/hadoop-project-without-template&amp;sa=D&amp;source=editors&amp;ust=1624552266911000&amp;usg=AOvVaw1580iQCIiROG-xcyMqAvt-">an example repository</a></span><span class="c3">&nbsp;using Hadoop in our documentation.</span></p><h1 class="c21" id="h.r9076n55opcc"><span class="c13">Description of the Work</span></h1><h2 class="c24" id="h.smijrirmrngb"><span class="c31">Preparation of the environments</span></h2><p class="c0"><span class="c3">First, we need to have a Renku and a Hadoop deployment of our own. Such deployments act as servers the user can access to use the services. &nbsp;For Renku, located on a single machine, access will be through a web UI. Hadoop&rsquo;s structure is more complicated, it is composed of several nodes that serve exclusively for the purpose of storage and computation.</span></p><h3 class="c25" id="h.e57lmohqmut4"><span class="c19">Renku deployment</span></h3><p class="c0"><span>A Renku deployment is a collection of Docker containers. Each serves a precise role: a server for the web UI, a container running the jupyter notebooks, etc. To facilitate the deployment and the management, Renku uses </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&amp;sa=D&amp;source=editors&amp;ust=1624552266913000&amp;usg=AOvVaw2KMTFOsnPXuoxowSa3bttj">Kubernetes</a></span><span>&nbsp;and </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://helm.sh/&amp;sa=D&amp;source=editors&amp;ust=1624552266913000&amp;usg=AOvVaw2i7oShFUuI1julM0mOZCCn">Helm</a></span><span>. Kubernetes is a system for automating the deployment of Docker containers. It is very useful in this context as the deployment is composed of more than a dozen containers plus one for each interactive environment running. Kubernetes centralizes the management of all containers so that they all are in the same service. It also manages the container automatically, restarting a container if it crashes for example. Helm is the package manager for Kubernetes. It permits the deployment of all containers needed by an application in one command. Helm is based on </span><span class="c22">charts</span><span>. A chart contains all the information needed by Kubernetes to deploy the application. The only missing component here is the Gitlab instance. As described in the introduction, Renku projects are just Git projects with additional information. So every Renku deployment is linked to a Gitlab instance where Renku stores all of its projects. It&#39;s up to the administrator of Renku to choose between using an existing Gitlab instance (part of Renku or not) or creating a new one, but be aware that choosing an instance external to Renku </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://renku.readthedocs.io/en/latest/developer/example-configurations/gitlab.html?%23caveats&amp;sa=D&amp;source=editors&amp;ust=1624552266914000&amp;usg=AOvVaw1t9f6BSuVdutP2b25-oH5j">comes with caveats</a></span><span>. Here we&rsquo;ll keep this part simple and just use the Gitlab instance of the public Renku instance: </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://renkulab.io/gitlab/team-renku&amp;sa=D&amp;source=editors&amp;ust=1624552266914000&amp;usg=AOvVaw29q4UlwimQR3f-IHj-qIWE">renkulab.io/gitlab</a></span><span>.</span></p><p class="c14 c1"><span class="c3"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 194.67px;"><img alt="" src="images/image1.png" style="width: 601.70px; height: 194.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14 c1"><span class="c5"></span></p><p class="c14"><span class="c7">Figure 1 - </span><span>Renku diagram</span><span class="c3">.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>So deploying Renku using Helm is very simple. The first step is to choose or deploy a Gitlab instance and then allow Renku to access it. Then, most of the work is in the creation of the configuration file for Helm. It is the only step that can be tricky. It is a YAML file, called </span><span class="c8">renku-values.yaml</span><span class="c3">, containing all the configuration of your deployment: the address where the UI will be accessible, the linked Gitlab instance, the methods of authentication, etc. With this file, Renku can be deployed using Renku&rsquo;s Helm chart. Once the deployment is successful, it is possible to configure Renku&rsquo;s Gitlab as an identity provider. It adds the possibility for the user to login into Renku using a Gitlab account.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>We don&rsquo;t go into too much detail here, for more information make sure to read </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://github.com/jjjules/renku-hadoop/tree/master/deployments/renku&amp;sa=D&amp;source=editors&amp;ust=1624552266916000&amp;usg=AOvVaw3sbIuN3AqDwKFuDpyQUESf">our documentation</a></span><span>&nbsp;where we describe in detail the steps we took for our deployment and </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://renku.readthedocs.io/en/latest/admin/index.html&amp;sa=D&amp;source=editors&amp;ust=1624552266917000&amp;usg=AOvVaw1R8z3VYYUevHHnkuCvaOIx">the official documentation</a></span><span>&nbsp;of Renku.</span></p><h3 class="c25" id="h.4ftsskc88lff"><span class="c19">Hadoop deployment</span></h3><p class="c0"><span>The deployment of Hadoop is much more complicated than the one of Renku. As it is distributed, it needs coordination between the nodes and without a service that facilitates the installations, all other services based on Hadoop would need manual installation, configuration and management. To automate this process we use </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://www.cloudera.com/products/open-source/apache-hadoop/apache-ambari.html&amp;sa=D&amp;source=editors&amp;ust=1624552266918000&amp;usg=AOvVaw0lA6LecvLczR5wxg6uHP7X">Ambari</a></span><span class="c3">, an application whose aim is to simplify the deployment and management of Hadoop clusters. Configured appropriately, Ambari can deploy the Hadoop clusters and install and configure all services the administrator wants to be available on the cluster.</span></p><p class="c0"><span>The problem with using Ambari is that </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://community.cloudera.com/t5/Support-Questions/How-do-I-download-the-latest-version-of-Ambari-and-HDP/m-p/299115/highlight/true%23M219531&amp;sa=D&amp;source=editors&amp;ust=1624552266918000&amp;usg=AOvVaw2yPM6GsX33EbUkdeDTggeO">it is no longer free</a></span><span>. </span><span>It is now mandatory to be subscribed to Cloudera to access the repository. The good news is that </span><span class="c22">only</span><span>&nbsp;accessing their repository needs a subscription. Their software was freely accessible before, so we can still access the code for free. All that is needed is to assemble it into a package so that we can use it easily. The other good news is that a Data Architect named </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://ds-steven-matison.github.io/&amp;sa=D&amp;source=editors&amp;ust=1624552266919000&amp;usg=AOvVaw0iIzfKfNF4lH8GjsbfBB35">Steven Matison</a></span><span>&nbsp;did exactly that. His site </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=http://makeopensourcegreatagain.com/&amp;sa=D&amp;source=editors&amp;ust=1624552266919000&amp;usg=AOvVaw2pLBrNmdRAXtB2vW7xttrQ">Make Open Source Great Again</a></span><span class="c3">&nbsp;(MOSGA) is composed of free repositories of software that become proprietary and it contains Ambari and its HDP and HDF software suites (Cloudera softwares related to Ambari). This allows us to use Ambari even in our free and open-source approach.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Now that we have access to the resources needed, we can deploy our Hadoop cluster. As for the Renku deployment, we broadly describe the deployment here but we&rsquo;ve written </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://github.com/jjjules/renku-hadoop/tree/master/deployments/hadoop&amp;sa=D&amp;source=editors&amp;ust=1624552266920000&amp;usg=AOvVaw0xE__Ol2UoF6y7pVmikePV">a detailed description</a></span><span>&nbsp;of the steps we took during the deployment in our documentation. And there are always </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://docs.cloudera.com/HDPDocuments/Ambari-2.7.4.0/bk_ambari-installation/content/ch_Getting_Ready.html&amp;sa=D&amp;source=editors&amp;ust=1624552266920000&amp;usg=AOvVaw1Ngh60uBFflbcMhhhRwtXA">the official instructions</a></span><span>&nbsp;that are more exhaustive. For the installation, Ambari starts from one node, the Ambari master, and can deploy itself to all other nodes named Ambari agents. A Hadoop deployment is installed directly on the machine, meaning that it doesn&rsquo;t use virtualization, as Renku does with Docker. Therefore here the first step is to install the machines with the correct Operating System. We install CentOS in order to be compatible with the MOSGA repositories. Then comes the preparation of the nodes: install the package dependencies, setup password-less connection via ssh between the Ambari master and the agents, etc. For the preparation of the nodes, one important step is to correctly set up both forward and reverse DNS lookup between the nodes. Because Hadoop relies heavily on DNS, it is important to have efficient DNS lookups. What we did is to simply hardcode the Domain/IP pairs in the </span><span class="c8">/etc/hosts</span><span>&nbsp;file on every host. Once all nodes are ready, it suffices to set up the Ambari server and start it, both using the </span><span class="c8">ambari-server</span><span class="c3">&nbsp;command. Then you can access the Ambari installation wizard at the domain name of your Ambari master. This wizard takes you through the installation process and if it is successful all following tasks (adding or modifying services, accessing configurations, etc.) can be done through the web UI of Ambari accessible through the same address as was the wizard (the IP address of the Ambari master).</span></p><h2 class="c24" id="h.dwzf3gsibeb9"><span class="c31">Hadoop setup in Renku</span></h2><h3 class="c25" id="h.fab7opk349hp"><span class="c19">Design</span></h3><p class="c0"><span class="c3">Our design should simplify the creation of Hadoop projects as much as possible without restricting the possible configurations. A beginner who has access to some Hadoop cluster should be able to create a project with the template and use the services without any additional configuration. But it is necessary to keep the possibility of having complicated configurations for the more advanced users. We need to separate completely the choice of the Hadoop cluster and the projects. Otherwise the issues mentioned previously (security, duplication across all projects and impossibility to share a project without sharing a back-end) will persist. Configuring the modules on the cluster should be optional, as in most cases a standard configuration suffices, but if needed it should be done in the same way as the choice of the back-end.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>The general idea is to have an adapted Docker image already set up to work with Hadoop and a script running at startup of each environment that reconfigures the modules if the user has a custom configuration and completes the configuration by linking the modules to the back-end. To store the configurations, we use the concept of </span><span class="c22">renku-env</span><span class="c3">&nbsp;repository. In Renku, if you have a repository on Gitlab named renku-env every file in it will be automatically copied to your home directory at the environment startup, or appended to the file with the same name if they already exist. We decide to store the choice of the back-end and the configuration of the back-end modules in it.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>To go into more details, all Renku environments start by creating the container based on the Docker image and the Dockerfile. The very last instruction of this process runs a script located at the root of the container named </span><span class="c8">entrypoint.sh</span><span>. It is a shell script that takes care of the dynamic configurations. Originally, it just sets up Git for the current user, imports the user&rsquo;s renku-env and clones the current project. For our use case, we use this script to produce the Hadoop configuration after pulling the renku-env. Our Docker image for Hadoop can be found </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://hub.docker.com/r/renkubigdata/renkulab-py-bigdata&amp;sa=D&amp;source=editors&amp;ust=1624552266923000&amp;usg=AOvVaw1nNxTP6BqQUbglduyVxGZ2">here</a></span><span class="c3">.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Concretely, the choice of the back-end is a list of key/value pairs that will be loaded as environment variables and used by the configuration files and the services. In our design this is a plain JSON file named </span><span class="c8">backend-conf.json</span><span>. This file can either be put directly in the renku-env or in an external repository. In that case, the repository should be given in that main Hadoop configuration file of the renku-env, named </span><span class="c8">hadoop-conf.json</span><span>. The configuration of the back-end modules cannot be contained in a file. Rather it is a collection of configuration files and environment variables. So this has to be in a separated repository. As for the choice of the backend, the repository should be indicated in the Hadoop configuration file of the renku-env. We describe the structure of this repository for the configuration of the modules. </span><span>In </span><span class="c7">Code 1</span><span>&nbsp;and </span><span class="c7">Code 2</span><span>&nbsp;we show the structure of the </span><span class="c8">hadoop-conf.json</span><span>&nbsp;file and our own </span><span class="c8">backend-conf.json</span><span class="c3">&nbsp;file.</span></p><p class="c0 c1"><span class="c2"></span></p><a id="t.1873358546ca0e48131b421f25aa00d3f81b6e73"></a><a id="t.0"></a><table class="c28"><tbody><tr class="c35"><td class="c11" colspan="1" rowspan="1"><p class="c0"><span class="c2">{</span></p><p class="c0"><span class="c26">&nbsp; &quot;</span><span class="c8">BACKEND_CONF</span><span class="c2">&quot;: &ldquo;&lt;URL-TO-BACKEND-CONF-FILE&gt;&rdquo;,</span></p><p class="c0"><span class="c26">&nbsp; &quot;</span><span class="c8">BACKEND_MODULES_CONF_REPO</span><span class="c2">&quot;: </span></p><p class="c0"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&ldquo;&lt;URL-TO-BACKEND-CONF-REPO-FOR-HADOOP-MODULES&gt;&rdquo;</span></p><p class="c0"><span class="c2">}</span></p></td></tr></tbody></table><p class="c0 c1"><span class="c2"></span></p><p class="c14"><span class="c7">Code 1 - </span><span>Structure of Hadoop main configuration file in renku-env</span><span>.</span></p><p class="c0 c1"><span class="c2"></span></p><a id="t.b79567645b9006256a0edce090f4d6e687b06ec7"></a><a id="t.1"></a><table class="c28"><tbody><tr class="c35"><td class="c11" colspan="1" rowspan="1"><p class="c0"><span class="c2">{</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;HADOOP_DEFAULT_FS_ARG&quot;: &quot;hdfs://hadoop-1.datascience.ch:8020&quot;,</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;HIVE_JDBC_ARG&quot;: &quot;jdbc:hive2://hadoop-3.datascience.ch:2181,hadoop-1.datascience.ch:2181,hadoop-2.datascience.ch:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2&quot;,</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;HIVE_SERVER_ARG&quot;: &quot;hadoop-2.datascience.ch&quot;,</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;HBASE_SERVER_ARG&quot;: &quot;hadoop-1.datascience.ch&quot;,</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;YARN_NM_HOSTNAME_ARG&quot;: &quot;hadoop-2.datascience.ch&quot;,</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;YARN_RM_HOSTNAME_ARG&quot;: &quot;hadoop-1.datascience.ch&quot;,</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;LIVY_SERVER_ARG&quot;: &quot;http://hadoop-1.datascience.ch:8999/&quot;</span></p><p class="c0"><span class="c2">&nbsp;}</span></p></td></tr></tbody></table><p class="c0 c1"><span class="c2"></span></p><p class="c14"><span class="c7">Code 2 - </span><span class="c3">Configuration file for the choice of back-end.</span></p><p class="c14 c1"><span class="c3"></span></p><p class="c0"><span>In our Hadoop deployment we have four nodes, hadoop-1 to hadoop-4 and as we can see in the </span><span class="c8">backend-conf.json</span><span class="c3">&nbsp;file only the first three appear. Meaning only the first three nodes contain entry points for services.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>In the repository for the configuration of the modules, there is a main file called </span><span class="c8">backend-modules-conf.json</span><span>&nbsp;containing the paths where the configuration file should be put in the Renku environment and environment variables the user wants to add or change. Adding an entry for a configuration file is optional for the following files as their location is already known: ???. These configuration files need information only available after the container startup, hence the user can write the names of such variables in the file as placeholder and these names will be replaced at startup with the appropriate values. The following variables (along with all new environment variables defined in </span><span class="c8">backend-modules-conf.json</span><span class="c3">) will be replaced in the configuration files:</span></p><ul class="c30 lst-kix_8cgkwypva7r0-0 start"><li class="c0 c15 li-bullet-0"><span class="c3">HDP_HOME, HADOOP_HOME and HIVE_HOME</span></li><li class="c0 c15 li-bullet-0"><span class="c3">HADOOP_CONF_DIR, HADOOP_DEFAULT_FS and HADOOP_USER_NAME</span></li><li class="c0 c15 li-bullet-0"><span class="c3">HIVE_JDBC_URL, HIVE_SERVER_2</span></li><li class="c0 c15 li-bullet-0"><span class="c3">HBASE_SERVER</span></li><li class="c0 c15 li-bullet-0"><span class="c3">YARN_NM_HOSTNAME, YARN_NM_ADDRESS, YARN_RM_HOSTNAME, YARN_RM_ADDRESS, YARN_RM_SCHEDULER and YARN_RM_TRACKER</span></li><li class="c0 c15 li-bullet-0"><span class="c3">LIVY_SERVER_URL</span></li><li class="c0 c15 li-bullet-0"><span class="c3">JAVA_HOME</span></li><li class="c0 c15 li-bullet-0"><span>JUPYTERLAB_DIR, JUPYTERLAB_SETTINGS_DIR and JUPYTERLAB_WORKSPACES_DIR</span></li></ul><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>For example, assume a user that wants to change his or her Hadoop username to </span><span class="c8">other-hadoop-name</span><span>&nbsp;and change the configuration of sparkmagic to gain access to more computing power. The repository for the modules&rsquo; configuration would have two files in it, </span><span class="c8">backend-modules-conf.json</span><span>&nbsp;and </span><span class="c8">sparkmagic.conf</span><span>, that could look as depicted in </span><span class="c7">Code 3</span><span>&nbsp;and </span><span class="c7">Code 4</span><span>. In </span><span class="c7">Code 4</span><span>, LIVY_SERVER_URL is an environment variable that will be replaced by its value at startup</span></p><p class="c0 c1"><span class="c3"></span></p><a id="t.5aa65a9536209e89b8072cb58a9f77a33249c0b1"></a><a id="t.2"></a><table class="c28"><tbody><tr class="c35"><td class="c11" colspan="1" rowspan="1"><p class="c0"><span class="c2">{</span></p><p class="c0"><span class="c2">&nbsp; &quot;CONF_FILES&quot;: {</span></p><p class="c0"><span class="c2">&nbsp; &nbsp; &quot;sparkmagic.conf&quot;: &quot;$HOME/.sparkmagic/config.json&quot;</span></p><p class="c0"><span class="c2">&nbsp; },</span></p><p class="c0"><span class="c2">&nbsp; &quot;ENV_VARIABLES&quot;: {</span></p><p class="c0"><span class="c2">&nbsp; &nbsp; &quot;HADOOP_USER_NAME&quot;: &quot;other-hadoop-name&quot; </span></p><p class="c0"><span class="c2">&nbsp; }</span></p><p class="c0"><span class="c26">}</span></p></td></tr></tbody></table><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span class="c7">Code 3 - </span><span>Example of the main configuration for modules in the back-end.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0 c1"><span class="c3"></span></p><a id="t.bd2a49b40ccffddae1114240456e6f72a5a419d1"></a><a id="t.3"></a><table class="c28"><tbody><tr class="c35"><td class="c11" colspan="1" rowspan="1"><p class="c0"><span class="c2">{</span></p><p class="c0"><span class="c2">&nbsp; &quot;kernel_python_credentials&quot; : {</span></p><p class="c0"><span class="c2">&nbsp; &nbsp; &quot;url&quot;: &quot;LIVY_SERVER_URL&quot;</span></p><p class="c0"><span class="c2">&nbsp; },</span></p><p class="c0"><span class="c2">&nbsp; &quot;kernel_scala_credentials&quot; : {</span></p><p class="c0"><span class="c2">&nbsp; &nbsp; &quot;url&quot;: &quot;LIVY_SERVER_URL&quot;</span></p><p class="c0"><span class="c2">&nbsp; },</span></p><p class="c0"><span class="c2">&nbsp; &quot;custom_headers&quot; : {</span></p><p class="c0"><span class="c2">&nbsp; &nbsp; &quot;X-Requested-By&quot;: &quot;livy&quot;</span></p><p class="c0"><span class="c2">&nbsp; },</span></p><p class="c0 c1"><span class="c2"></span></p><p class="c0"><span class="c2">&nbsp; &quot;session_configs&quot; : {</span></p><p class="c0"><span class="c2">&nbsp; &nbsp; &quot;driverMemory&quot;: &quot;10000M&quot;,</span></p><p class="c0"><span class="c2">&nbsp; &nbsp; &quot;executorMemory&quot;: &quot;42G&quot;,</span></p><p class="c0"><span class="c2">&nbsp; &nbsp; &quot;executorCores&quot;: 16,</span></p><p class="c0"><span class="c2">&nbsp; &nbsp; &quot;numExecutors&quot;: 32</span></p><p class="c0"><span class="c2">&nbsp; },</span></p><p class="c0 c1"><span class="c2"></span></p><p class="c0"><span class="c2">&nbsp; &quot;server_extension_default_kernel_name&quot;: &quot;pysparkkernel&quot;,</span></p><p class="c0"><span class="c2">&nbsp; &quot;use_auto_viz&quot;: true,</span></p><p class="c0"><span class="c2">&nbsp; &quot;coerce_dataframe&quot;: true,</span></p><p class="c0"><span class="c2">&nbsp; &quot;max_results_sql&quot;: 1000,</span></p><p class="c0"><span class="c2">&nbsp; &quot;pyspark_dataframe_encoding&quot;: &quot;utf-8&quot;,</span></p><p class="c0"><span class="c2">&nbsp; &quot;heartbeat_refresh_seconds&quot;: 5,</span></p><p class="c0"><span class="c2">&nbsp; &quot;livy_server_heartbeat_timeout_seconds&quot;: 60,</span></p><p class="c0"><span class="c2">&nbsp; &quot;heartbeat_retry_seconds&quot;: 1</span></p><p class="c0"><span class="c2">}</span></p></td></tr></tbody></table><p class="c0 c1"><span class="c3"></span></p><p class="c14"><span class="c7">Code 4 - </span><span>Example </span><span class="c3">configuration for spark magic.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span class="c3">Now there are several flaws to this approach. If the only possibility for configuring Hadoop is through the user&rsquo;s renku-env, the automatic detection of a back-end the user has access to is not possible. Ideally, if a user takes part in a course that gives him or her access to a cluster, we should have a mechanism to detect this and use the back-end of the course automatically. This would avoid duplicating the back-end configuration for all students of that course and in case the back-end changes the administrator would just have to adapt the configuration of the course. We can achieve this by using the concept of groups in Gitlab. A Gitlab group is an entity that englobes a collection of users. Repositories can be created under a group and so we can have a renku-env for groups also. Using this, we can have a back-end (choice of back-end and its configuration) for a group and thus automatically detect the cluster the user has access to using his or her belonging to Gitlab groups. At startup the script will look at the available configuration in the user&rsquo;s groups, override them with the custom configuration of the user if there is any and load the resulting configuration into the environment.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span class="c3">This final design fulfills all the objectives we wanted to achieve. A user belonging to a group with a proper back-end configuration can use the template without any additional effort. A user can choose his or her own configuration, re-use another person&rsquo;s configuration for the modules by just providing the address in the renku-env or make his or her own configuration. The separation of the project and the back-end eases the maintenance by avoiding duplication. Maintaining the template will also require a minimal amount of work as only the Docker image has to be updated depending on the package&#39;s releases. It allows users to easily share a cluster or their cluster configuration. Using it, Renku could for example give access to clusters as a service easily. All that is needed would be to put the user in the appropriate Gitlab group.</span></p><h3 class="c25" id="h.5hu4v9wss5xa"><span class="c19">Issues and possible improvements:</span></h3><p class="c0"><span>The interactive environment of Renku</span><span>&nbsp;is not trusted, so the credentials for Gitlab are not present inside. Otherwise, a malicious user could hide scripts running at startup in a project and get full access to the person&rsquo;s repositories. From that point, the malicious user could delete all repositories of the user, publish them publicly on the internet or other undesirable outcomes. For this reason, the communication between Renku and Gitlab goes through a sidekick that acts as a simple proxy. Only this sidekick has access to the credentials of the Gitlab account. What this proxy does is to add the credentials only if the request (clone, pull, push, etc.) points to the repository from which the Renku environment has been launched.</span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 301.33px;"><img alt="" src="images/image2.png" style="width: 601.70px; height: 301.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14 c1"><span class="c5"></span></p><p class="c14"><span class="c7">Figure 2 - </span><span>Diagram of Renku&rsquo;s communication.</span></p><p class="c14 c1"><span class="c3"></span></p><p class="c0"><span>This is problematic for us because we need access to the user&rsquo;s groups and back-end repositories, whether they are public or private. Currently, even the renku-env has to be public to be pulled. Moreover, for obvious security reasons it is important to be able to make the renku-env private for our design, otherwise we expose our cluster. It is possible to modify the sidekick so that it gives access to, and only, the needed Git resources. We only need read-only access to the user&rsquo;s group names, the renku-env&rsquo;s and the repositories for the back-end. It makes sense to give access to the user&rsquo;s group names and the renku-env&rsquo;s but we have to restrict the access for the repositories of the back-end. For them, we could reserve names, for example </span><span class="c8">renku-hadoop-backend</span><span>&nbsp;and</span><span>&nbsp;</span><span class="c8">renku-hadoop-modules-conf</span><span>, and only give read access to those. It is true that if a user creates a repository with the same name, we would give access to it without him or her knowing but the chance of collision is very small. We haven&rsquo;t modified the sidekick as just described so in the current state of the design all used groups and repositories must be public. The reason is that an active contributor of Renku told us that storing user settings in Gitlab was &ldquo;pretty clumsy&rdquo; and so it will most likely not be kept in Renku in the long term. The best alternative to our storing design may be to use a dedicated Renku component. In the long term, it would also permit access to these configurations directly in the web UI of Renku. As this storage will most likely change, we separate the entrypoint script into a </span><span class="c8">get_configuration.sh</span><span>&nbsp;script and a </span><span class="c8">&nbsp;load_configuration.sh</span><span>&nbsp;script so that in the future only the former needs to be modified.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>To improve the user experience, we tried to add sensible defaults to the python functions of the different services. For example for Hive, the user needs to create a Hive connection with </span><span class="c8">hive.connect</span><span>&nbsp;and give it the address of the Hive server and a username. We thought it would be helpful to set the defaults of this function, as well as similar ones for other services, as we have this information available in the environment. So that if the user calls </span><span class="c8">hive.connect</span><span>&nbsp;without any argument, the function uses the correct Hive address and the username of the current user. The problem is that there is no support to reassign defaults of functions in python. The only sensible other alternative that doesn&rsquo;t involve changing the source code directly would be to define a new function, e.g. </span><span class="c8">hiveconnector</span><span>, available in all interactive contexts that would take care of the defaults if the arguments are missing. There is the possibility in jupyter to add a python script running at the startup of each interactive python environment where we could define this function (</span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://ipython.readthedocs.io/en/stable/interactive/tutorial.html?%23startup-files&amp;sa=D&amp;source=editors&amp;ust=1624552266946000&amp;usg=AOvVaw0wvFe5XFIquPNe5D_LeiQk">source</a></span><span>). But there are two main problems with this approach. First, It can mislead the user into thinking that this function is part of the codebase of Hive or another package. And more importantly, the code would not be portable. It wouldn&rsquo;t work anywhere outside of Renku, which from a reproducibility perspective is not acceptable.</span></p><h3 class="c25" id="h.8xl93iqhiej1"><span class="c19">Authentication to Hadoop services</span></h3><p class="c0"><span>Our Hadoop deployment does not include any form of authentication, neither to connect to the cluster nor to the services on the cluster. Not only is this a severe security issue but problems arise also in the management of the cluster because of that. We need the processes to be launched or at least linked in a way to the users, otherwise management becomes much harder. We can&rsquo;t easily kill all processes of a user for example. Moreover, without authentication not only the cluster is unsafe from people without proper access (abuse of resources and attacks) but also are the resources between allowed users. Even unintentionally, one could easily delete resources of other users.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span class="c3">This motivates us to improve our deployment so as to provide both identification of the users to each of their processes and authentication to the cluster and to the services where it is needed. It is important to say that, unlike all other work we present here, part of what we present is still in the designing stage. Meaning that we&#39;ve not deployed it to our cluster. Hence, we cannot provide complete documentation for these parts but rather explain our approach and show our research about the design.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>There are services available in Ambari that can help us for the authentication, namely </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://www.cloudera.com/products/open-source/apache-hadoop/apache-knox.html&amp;sa=D&amp;source=editors&amp;ust=1624552266948000&amp;usg=AOvVaw1WI7fC55URa_i3mTqXEoUY">Apache Knox</a></span><span>&nbsp;and </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://www.cloudera.com/products/open-source/apache-hadoop/apache-ranger.html&amp;sa=D&amp;source=editors&amp;ust=1624552266949000&amp;usg=AOvVaw37P9HVuFOEqTaXIiD_z5Oi">Apache Ranger</a></span><span>. Knox</span><span class="c3">&nbsp;is a perimeter security gateway system for the cluster. Meaning that only users successfully authenticated will be given access to the cluster. Additionally, Knox provides a layer of abstraction to the Hadoop services, every service endpoint inside the cluster is accessed via Knox API which already permits identification of users. Ranger is an authorization system handling the access to the cluster resources (HDFS, Hive tables, etc.). Ranger is complementary to Knox, to make a request to Ranger the user has to be authenticated already. The majority of services available in the cluster have Ranger plugins integrated to them allowing easy integration.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 546.48px; height: 333.15px;"><img alt="" src="images/image3.png" style="width: 546.48px; height: 333.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c14"><span class="c7">Figure 3 -</span><span class="c3">&nbsp;Knox and Ranger diagram.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>We propose to use these two services with </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://stealthbits.com/blog/what-is-kerberos/&amp;sa=D&amp;source=editors&amp;ust=1624552266950000&amp;usg=AOvVaw2_XC0YlacgwQ5IUizhgt0f">Kerberos</a></span><span>&nbsp;as the security protocol. Kerberos has been shown to be very effective and secure and it is the default authorization technology in many domains. For that, first we have to set up a Key Distribution Center (KDC) on the Hadoop cluster. We did it by following </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://godatadriven.com/blog/kerberos-basics-and-installing-a-kdc/&amp;sa=D&amp;source=editors&amp;ust=1624552266951000&amp;usg=AOvVaw3vNiaz1_8gVmaut0xeuNAB">this tutorial</a></span><span>&nbsp;and as always provide the produced configuration files and additional instructions in </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://github.com/jjjules/renku-hadoop/tree/master/authentication/KDC-setup&amp;sa=D&amp;source=editors&amp;ust=1624552266951000&amp;usg=AOvVaw1dLlXR50XugrVj3fHeyD-S">our documentation</a></span><span class="c3">. Then, Knox and Ranger can be installed directly from the Ambari UI and once installed these services provide their own web UI allowing easy setup and management of users.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>That being said, we tried to install Knox several times and with every installation the web UI crashes after a few clicks which makes the setup of the policies painful. As for Ranger, we couldn&#39;t install the software using Ambari as it depends on Ambari Infra Solr, an indexing service, which fails to install. The reason is that the script used by Ambari expects a repository not present on the MOSGA repositories. To fix this we should make a copy of them and add the needed packages.</span></p><h1 class="c21" id="h.jo1qk0s25skm"><span class="c13">Results</span></h1><p class="c0"><span>We show here the resulting setup for creating a Renku project using Hadoop using our design on our deployments. We describe the resulting template and how it works. This example repository can be found </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://github.com/jjjules/renku-hadoop/tree/master/hadoop-project-with-template&amp;sa=D&amp;source=editors&amp;ust=1624552266952000&amp;usg=AOvVaw22zZrgDtNsDg_kI6JbJCoO">here</a></span><span class="c3">.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>In the project, the only difference between the Hadoop template and a standard python template is the Docker image used as the entrypoint script is already present at the root of it. The Docker image handles all dependencies and static configuration of the machine needed to work with Hadoop and the entrypoint script handles the dynamic configuration of the back-end and the linking between the Renku environment and the back-end. Hence, starting from a standard python Renku project we just need to change the Docker image for our image for Hadoop. As a side note, for the purpose of development, to work on the entrypoint script one can add it at the root of the project and add the following lines just before the </span><span class="c22">do not edit</span><span>&nbsp;section at </span><span class="c3">of the Dockerfile:</span></p><p class="c0 c1"><span class="c3"></span></p><a id="t.aedc16d088e5cf5a8874cdc96f5a5dd053dc8e7e"></a><a id="t.4"></a><table class="c28"><tbody><tr class="c35"><td class="c11" colspan="1" rowspan="1"><p class="c0"><span class="c2"># Use custom entrypoint.sh if it exists</span></p><p class="c0"><span class="c2">USER root</span></p><p class="c0"><span class="c2">COPY entrypoint.sh /tmp/entrypoint.sh</span></p><p class="c0"><span class="c2">RUN if [ -f /tmp/entrypoint.sh ]; then mv /entrypoint.sh /entrypoint.sh.backup; mv /tmp/entrypoint.sh /; fi</span></p><p class="c0"><span class="c2">USER ${NB_USER}</span></p></td></tr></tbody></table><p class="c0 c1"><span class="c3"></span></p><p class="c14"><span class="c7">Code 5 -</span><span>&nbsp;Modifying the used </span><span class="c8">entrypoint.sh</span><span class="c3">.</span></p><p class="c14 c1"><span class="c3"></span></p><p class="c0"><span>No other modification is needed for the project, but we need a back-end registered in Gitlab. We will define the back-end in our personal user-env here, but keep in mind that if this back-end were configured in a (public) group we belonged to, it would work just the same. For the first possibility of configuration, we put the </span><span class="c8">backend-conf.json</span><span>&nbsp;directly at the root of the renku-env. The content of this file is shown in </span><span class="c7">Code 2</span><span>. Now that a back-end is present in Gitlab, we can launch an interactive environment and start to code using the services available on the cluster. For the sake of example, we show a trivial code snippet runned in this example project in </span><span class="c7">Code 6</span><span class="c3">.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>To find the values for the fields in the </span><span class="c8">backend-conf.json</span><span class="c3">&nbsp;file, the simplest solution is to go to the Ambari dashboard and get the information from there. The mapping between names in the file and in the Ambari dashboard is:</span></p><ul class="c30 lst-kix_ocz5h2tob8yz-0 start"><li class="c0 c15 li-bullet-0"><span class="c8">HADOOP_DEFAULT_FS_ARG</span><span>: </span><span class="c8">fs.defaultFS</span><span class="c3">&nbsp;value in HDFS advanced configuration </span></li><li class="c0 c15 li-bullet-0"><span class="c8">HIVE_JDBC_ARG</span><span>: get the value </span><span class="c8">hive.llap.zk.sm.connectionString</span><span>&nbsp;in the advanced configuration of Hive, then the value for </span><span class="c8">HIVE_JDBC_ARG</span><span>&nbsp;is obtained by</span><span class="c8">&nbsp;</span><span>&nbsp;substituting this value into </span><span class="c8">jdbc:hive2://&lt;hive.llap.zk.sm.connectionString&gt;;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup></li><li class="c0 c15 li-bullet-0"><span class="c8">HIVE_SERVER_ARG</span><span>: </span><span class="c22">Hive Metastore host</span><span class="c3">&nbsp;in the advanced configuration</span></li><li class="c0 c15 li-bullet-0"><span class="c8">HBASE_SERVER_ARG</span><span>: </span><span class="c22">Hbase Master host</span><span class="c3">&nbsp;in its advanced configuration</span></li><li class="c0 c15 li-bullet-0"><span class="c8">YARN_NM_HOSTNAME_ARG</span><span>: </span><span class="c22">NodeManager host</span><span class="c3">&nbsp;in YARN&rsquo;s advanced configuration</span></li><li class="c0 c15 li-bullet-0"><span class="c8">YARN_RM_HOSTNAME_ARG</span><span>: </span><span class="c22">ResourceManager host</span><span class="c3">&nbsp;in YARN&rsquo;s advanced configuration</span></li><li class="c0 c15 li-bullet-0"><span class="c8">LIVY_SERVER_ARG</span><span>: In Spark configuration summary, under </span><span class="c22">Livy Server</span></li></ul><p class="c0 c1"><span class="c3"></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 540.00px;"><img alt="" src="images/image5.png" style="width: 601.70px; height: 540.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span class="c7">Code 6- </span><span class="c3">Simple code example ran with the new design.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Now, if we want to add custom configuration to services on the cluster, we can do it by providing a configuration repository in the </span><span class="c8">hadoop-conf.json</span><span>&nbsp;under the key </span><span class="c8">BACKEND_MODULES_CONF_REPO</span><span>&nbsp;in the renku-env. We also show this mode of configuration for the choice of the backend, by providing the </span><span class="c8">backend-conf.json</span><span>&nbsp;in an external repository, under the key </span><span class="c8">BACKEND_CONF_REPO</span><span>. The structure of the resulting </span><span class="c8">hadoop-conf.json</span><span>&nbsp;is shown in </span><span class="c7">Code 7</span><span class="c3">.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0 c1"><span class="c3"></span></p><a id="t.236a556e3246dc0cffa791eceb354e31664394ff"></a><a id="t.5"></a><table class="c28"><tbody><tr class="c37"><td class="c11" colspan="1" rowspan="1"><p class="c0"><span class="c2">{</span></p><p class="c0"><span class="c26">&nbsp; &quot;</span><span class="c8">BACKEND_CONF_REPO</span><span class="c2">&quot;: &ldquo;https://&lt;git-repository-containing-backend-conf.json&gt;&rdquo;,</span></p><p class="c0"><span class="c26">&nbsp; &quot;</span><span class="c8">BACKEND_MODULES_CONF_REPO</span><span class="c2">&quot;: &ldquo;https://&lt;git-repository-containing-modules-configuration&gt;&rdquo;</span></p><p class="c0"><span class="c2">}</span></p></td></tr></tbody></table><p class="c0 c1"><span class="c3"></span></p><p class="c14"><span class="c7">Code 7 -</span><span>&nbsp;New Hadoop configuration in renku-env (</span><span class="c8">hadoop-conf.json</span><span class="c3">).</span></p><p class="c14 c1"><span class="c3"></span></p><p class="c0"><span>As for the example in the design, let&#39;s just modify the Hadoop username and the sparkmagic configuration. The modules&#39; configuration repository hence contains the files shown in </span><span class="c7">Code 3</span><span>&nbsp;and </span><span class="c7">Code 4</span><span class="c3">.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>A summary of the methods of configuration is shown in </span><span class="c7">Figure 4</span><span>. </span><span class="c3">One last note about the results obtained. Our design fully worked on another deployment but some services (Spark and HBase) failed to work on our own deployment. The issue seems to be related to the deployment rather than the design as it worked on the other Renku instance. But for example, HBase is working when using the shell directly on the cluster on our own deployment but even with the correct entry point address the Renku environment fails to connect to it. We either have a connectivity problem (e.g. misconfiguration in the firewall) or the HBase server is listening to another port or not listening at all. So there may be additional configuration needed on the cluster to be able to link the Renku instance with them as our design does.</span></p><p class="c0 c1"><span class="c3"></span></p><p class="c1 c14"><span class="c5"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 345.33px;"><img alt="" src="images/image4.png" style="width: 601.70px; height: 345.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span class="c7">Figure 5 - </span><span class="c3">Diagram showing different methods of back-end configuration.</span></p><p class="c0 c1"><span class="c3"></span></p><hr style="page-break-before:always;display:none;"><h1 class="c16" id="h.8mg34hm2jw2i"><span class="c13"></span></h1><h1 class="c21" id="h.4f252vy00wzy"><span class="c13">Conclusion</span></h1><p class="c0"><span>In this project, we explored how to deploy and configure Data Science projects with Hadoop in Renku. We incorporate the configuration between the two components in our design of template for Renku. This template permits to reduce as much as possible the configuration needed to use a Hadoop cluster. Just from the file that defines the backend (5 to 10 addresses) it can automatize the rest of the procedure while keeping the possibility of configuring further the cluster&#39;s components. We also began working on an important configuration element of a Hadoop cluster, namely to provide authentication of users of it and the services part of it.</span><hr style="page-break-before:always;display:none;"></p><h1 class="c21" id="h.3chbr58lydt"><span>References</span></h1><p class="c0"><span>Documentation for the project (our documentation for deployments, design of Hadoop setup in Renku, etc.): </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://github.com/JulesssG/renku-hadoop&amp;sa=D&amp;source=editors&amp;ust=1624552266966000&amp;usg=AOvVaw06JMUG2jcmHg_6JJM_cnTZ">https://github.com/jjjules/renku-hadoop</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Apache website:</span><span class="c12">&nbsp;</span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://www.apache.org/&amp;sa=D&amp;source=editors&amp;ust=1624552266967000&amp;usg=AOvVaw3TetD3W0Gd_eG8rCeqONWH">https://www.apache.org/</a></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Hadoop description: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://www.lebigdata.fr/hadoop&amp;sa=D&amp;source=editors&amp;ust=1624552266968000&amp;usg=AOvVaw27vIU2YZxYW6VC0Ayzqza7">https://www.lebigdata.fr/hadoop</a></span><span>, </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://searchdatamanagement.techtarget.com/definition/Hadoop&amp;sa=D&amp;source=editors&amp;ust=1624552266968000&amp;usg=AOvVaw0xzo3N8fM7KlSmvnvhseAx">https://searchdatamanagement.techtarget.com/definition/Hadoop</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>What is Spark?: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://www.infoworld.com/article/3236869/what-is-apache-spark-the-big-data-platform-that-crushed-hadoop.html&amp;sa=D&amp;source=editors&amp;ust=1624552266969000&amp;usg=AOvVaw2tWlSrdapN_vLHk82x2Gup">https://www.infoworld.com/article/3236869/what-is-apache-spark-the-big-data-platform-that-crushed-hadoop.html</a></span></p><p class="c0 c1"><span class="c32"></span></p><p class="c0"><span>Kubernetes overview: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&amp;sa=D&amp;source=editors&amp;ust=1624552266970000&amp;usg=AOvVaw2HMyRAXiwO90BzxzhkmlYv">https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Helm: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://helm.sh/&amp;sa=D&amp;source=editors&amp;ust=1624552266970000&amp;usg=AOvVaw0MReqdB9ULBIOYaLwRpZfu">https://helm.sh/</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Swiss Data Science Center:</span><span class="c12">&nbsp;</span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://datascience.ch/renku/&amp;sa=D&amp;source=editors&amp;ust=1624552266971000&amp;usg=AOvVaw37k1VB-p-_kn9yMpp3lTSH">https://datascience.ch/</a></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Renku:</span><span class="c12">&nbsp;</span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://datascience.ch/renku/&amp;sa=D&amp;source=editors&amp;ust=1624552266971000&amp;usg=AOvVaw37k1VB-p-_kn9yMpp3lTSH">https://datascience.ch/renku/</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Renku source code:</span><span class="c12">&nbsp;</span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://github.com/SwissDataScienceCenter/renku&amp;sa=D&amp;source=editors&amp;ust=1624552266972000&amp;usg=AOvVaw2lSTfWZqKwtA3mp3KYCBxc">https://github.com/SwissDataScienceCenter/renku</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Renku documentation: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://renku.readthedocs.io/en/latest/introduction/why.html&amp;sa=D&amp;source=editors&amp;ust=1624552266973000&amp;usg=AOvVaw3L-lxcshgivavv9SpDtFxS">https://renku.readthedocs.io/en/latest/introduction/why.html</a></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Caveats of using an external Gitlab with Renku: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://renku.readthedocs.io/en/latest/developer/example-configurations/gitlab.html?%23caveats&amp;sa=D&amp;source=editors&amp;ust=1624552266973000&amp;usg=AOvVaw0WldR-O9UJTxEYNaL__jow">https://renku.readthedocs.io/en/latest/developer/example-configurations/gitlab.html?#caveats</a></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Public Renku deployment: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://renkulab.io&amp;sa=D&amp;source=editors&amp;ust=1624552266974000&amp;usg=AOvVaw1RoNabpD_csbMfw9DYtsYk">https://renkulab.io</a></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Public Renku deployment&rsquo;s Gitlab: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://renkulab.io&amp;sa=D&amp;source=editors&amp;ust=1624552266975000&amp;usg=AOvVaw29lNLrv7QxuXfLNne2M4cF">https://renkulab.io/gitlab</a></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Official documentation for Renku&rsquo;s deployment: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://renku.readthedocs.io/en/latest/admin/index.html&amp;sa=D&amp;source=editors&amp;ust=1624552266975000&amp;usg=AOvVaw0f3ZyNsal6M22kdpL9Xf1J">https://renku.readthedocs.io/en/latest/admin/index.html</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Hadoop Docker image:</span><span class="c12">&nbsp;</span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://hub.docker.com/r/renkubigdata/renkulab-py-bigdata&amp;sa=D&amp;source=editors&amp;ust=1624552266976000&amp;usg=AOvVaw1A5AUh4uS69qK4LLd4JlV4">https://hub.docker.com/r/renkubigdata/renkulab-py-bigdata</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Thread discussing free access to Ambari: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://community.cloudera.com/t5/Support-Questions/How-do-I-download-the-latest-version-of-Ambari-and-HDP/m-p/299115/highlight/true%23M219531&amp;sa=D&amp;source=editors&amp;ust=1624552266977000&amp;usg=AOvVaw3tdwa2IDDqX5G8dBPd7uqr">https://community.cloudera.com/t5/Support-Questions/How-do-I-download-the-latest-version-of-Ambari-and-HDP/m-p/299115/highlight/true#M219531</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Ambari: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://www.cloudera.com/products/open-source/apache-hadoop/apache-ambari.html&amp;sa=D&amp;source=editors&amp;ust=1624552266978000&amp;usg=AOvVaw3NyTiLDJUy05MPut6DKJRR">https://www.cloudera.com/products/open-source/apache-hadoop/apache-ambari.html</a></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Steven Matison&rsquo;s information (author of MOSGA): </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://ds-steven-matison.github.io/&amp;sa=D&amp;source=editors&amp;ust=1624552266979000&amp;usg=AOvVaw03s2h0zpyZx_lKJ_Gv9pbY">https://ds-steven-matison.github.io/</a></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>MOSGA repositories: </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=http://makeopensourcegreatagain.com/&amp;sa=D&amp;source=editors&amp;ust=1624552266981000&amp;usg=AOvVaw2f5OL5Y2xugWAPm1laGcFv">http://makeopensourcegreatagain.com/</a></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Cloudera&rsquo;s documentation for Hadoop deployment: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://docs.cloudera.com/HDPDocuments/Ambari-2.7.4.0/bk_ambari-installation/content/ch_Getting_Ready.html&amp;sa=D&amp;source=editors&amp;ust=1624552266982000&amp;usg=AOvVaw3cWb0hyHutOdBYq_d57Etg">https://docs.cloudera.com/HDPDocuments/Ambari-2.7.4.0/bk_ambari-installation/content/ch_Getting_Ready.html</a></span></p><p class="c0 c1"><span class="c3"></span></p><p class="c0"><span>Documentation for IPython&rsquo;s startup script: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://ipython.readthedocs.io/en/stable/interactive/tutorial.html?%23startup-files&amp;sa=D&amp;source=editors&amp;ust=1624552266983000&amp;usg=AOvVaw2oLmCwva_xC37M03A1vz9U">https://ipython.readthedocs.io/en/stable/interactive/tutorial.html?#startup-files</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Apache Knox documentation: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://www.cloudera.com/products/open-source/apache-hadoop/apache-knox.html&amp;sa=D&amp;source=editors&amp;ust=1624552266984000&amp;usg=AOvVaw0Zsv01aUWQhzOpJQuVMsY6">https://www.cloudera.com/products/open-source/apache-hadoop/apache-knox.html</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Apache Ranger: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://www.cloudera.com/products/open-source/apache-hadoop/apache-ranger.html&amp;sa=D&amp;source=editors&amp;ust=1624552266985000&amp;usg=AOvVaw3mf4JK2lQQNyBje03vcgy-">https://www.cloudera.com/products/open-source/apache-hadoop/apache-ranger.html</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Difference between Apache Knox and Ranger: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://community.cloudera.com/t5/Support-Questions/whats-the-difference-between-Ranger-and-Knox/td-p/159565&amp;sa=D&amp;source=editors&amp;ust=1624552266986000&amp;usg=AOvVaw2F5W-1uSAYIr2dr7C5_5nI">https://community.cloudera.com/t5/Support-Questions/whats-the-difference-between-Ranger-and-Knox/td-p/159565</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>Setting up a KDC for Kerberos: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://godatadriven.com/blog/kerberos-basics-and-installing-a-kdc/&amp;sa=D&amp;source=editors&amp;ust=1624552266987000&amp;usg=AOvVaw2ydt68E_yd6zXD5sKWDWbR">https://godatadriven.com/blog/kerberos-basics-and-installing-a-kdc/</a></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0"><span>How does Kerberos works?: </span><span class="c9 c12"><a class="c6" href="https://www.google.com/url?q=https://stealthbits.com/blog/what-is-kerberos/&amp;sa=D&amp;source=editors&amp;ust=1624552266988000&amp;usg=AOvVaw3-dnxAtdy_4I5ZB47vpCsy">https://stealthbits.com/blog/what-is-kerberos/</a></span></p><div><p class="c1 c39"><span class="c3"></span></p></div><hr class="c33"><div><p class="c23"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c4">&nbsp;This string will depend of your personal configuration, whether or not you are using ZooKeeper for to discover your services for example.</span></p></div></body></html>