{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with HDFS, Hive and HBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "hiveaddr = os.environ['HIVE_SERVER_2']\n",
    "print(\"Operating as: {0}\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs3 import HDFileSystem\n",
    "hdfs = HDFileSystem()\n",
    "hdfs.ls(\"/user/{0}\".format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## HDFS / Hive Storage format\n",
    "\n",
    "Hive gives you the ability to choose between many storage formats for your data. You can find a list in the [Hive Storage Formats](https://cwiki.apache.org/confluence/display/Hive/FileFormats) documentation ([references](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-StorageFormats))\n",
    "\n",
    "Among them, the most commonly used formats are _textfile_, _parquet_ and _Optmized Row Columnar (ORC)_.\n",
    "\n",
    "The _textfile_ format should be used mostly for external files that are already in text format, or to make the tables available as external files to machines or humans who are not able to process other formats. They are mostly seen at the _edges_ of your big-data processing pipeline, which is where the data comes in, or out, and where interoperability with external sources or data consumers is required.\n",
    "\n",
    "In other situations, we will prefer the _parquet_ or _ORC_ format, which are optimized to store Hive data more efficiently. In particular, _parquet_ or _ORC_ should be your first choice for _Hive-managed_ tables, or temporary tables.\n",
    "\n",
    "We will import data from our HDFS storage that contains data from the SBB.\n",
    "\n",
    "#### Converting from _textfile_ to _ORC_\n",
    "\n",
    "When receiving a file in a text format, sometimes it makes sense to create a copy using the more efficient storage formats _parquet_ or _ORC_. Transforming the data into these formats has a (CPU) cost, however it only needs to be paid once, when the tables are created, and it is largely paid back by the read performance boost we get from them. Furtheremore, those formats are understood by many other utilities of the big data platform, such as Spark. We thus want to store this data in an _external_ tables, in a location that is accessible to those utilities.\n",
    "\n",
    "We illustrate the format conversion in 3 steps in the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "We must first create a connection to the Hive server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "\n",
    "# create connection\n",
    "conn = hive.connect(host=hiveaddr, \n",
    "                    port=10000,\n",
    "                    username=username) \n",
    "# create cursor\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Assume you have received the sbb data in CSV format and that this data is now stored in HDFS. In this exercise we use a subset of the sbb data (December 2020), which you can find under `/data/sbb/istdaten/2020/12` on HDFS.\n",
    "\n",
    "**Step 1.** Create an [_external table_](https://cwiki.apache.org/confluence/display/Hive/Managed+vs.+External+Tables) stored in _textfile_ format, located on `/data/sbb/istdate/2019/12`.\n",
    "\n",
    "* Create a database using your name\n",
    "\n",
    "* Make the database your default with `use`.\n",
    "\n",
    "* Drop the table if it exists\n",
    "\n",
    "* Create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    create database if not exists {0}\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "query = \"\"\"\n",
    "    use {0}\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    drop table if exists {0}.sbb_csv_2020_12\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    create external table {0}.sbb_csv_2020_12(\n",
    "        BETRIEBSTAG string,\n",
    "        FAHRT_BEZEICHNER string,\n",
    "        BETREIBER_ID string,\n",
    "        BETREIBER_ABK string,\n",
    "        BETREIBER_NAME string,\n",
    "        PRODUKT_ID string,\n",
    "        LINIEN_ID string,\n",
    "        LINIEN_TEXT string,\n",
    "        UMLAUF_ID string,\n",
    "        VERKEHRSMITTEL_TEXT string,\n",
    "        ZUSATZFAHRT_TF string,\n",
    "        FAELLT_AUS_TF string,\n",
    "        BPUIC string,\n",
    "        HALTESTELLEN_NAME string,\n",
    "        ANKUNFTSZEIT string,\n",
    "        AN_PROGNOSE string,\n",
    "        AN_PROGNOSE_STATUS string,\n",
    "        ABFAHRTSZEIT string,\n",
    "        AB_PROGNOSE string,\n",
    "        AB_PROGNOSE_STATUS string,\n",
    "        DURCHFAHRT_TF string\n",
    "    )\n",
    "    row format delimited fields terminated by ';'\n",
    "    stored as textfile\n",
    "    location '/data/sbb/csv/istdaten/2020/12/'\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the header line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"\"\"\n",
    "alter table {0}.sbb_csv_2020_12 set tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify external table `sbb_csv_2020_12`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from {0}.sbb_csv_2020_12 limit 5\n",
    "\"\"\".format(username)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Step 2.** Create a new table in your HDFS folder, under `/user/{0}/hive/sbb/orc`.\n",
    "\n",
    "Notes:\n",
    "* We store this data in [ORC](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-HiveQLSyntax) storage format\n",
    "* We use `TBLPROPERTIES` to set the compression mode to `SNAPPY`\n",
    "* The table is external. If we drop the table, the generated _ORC_ files will still be available to other big data applications.\n",
    "* It is a new table, and it is empty. We will insert data into it.\n",
    "* Hive will create the folder at the specified location on HDFS if it does not exist. Because we are all going to create this new table, we do not want to write over each other data, we will therefore locate this external table into our HDFS home folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    drop table if exists {0}.sbb_orc_2020_12\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    create external table {0}.sbb_orc_2020_12(\n",
    "        BETRIEBSTAG string,\n",
    "        FAHRT_BEZEICHNER string,\n",
    "        BETREIBER_ID string,\n",
    "        BETREIBER_ABK string,\n",
    "        BETREIBER_NAME string,\n",
    "        PRODUKT_ID string,\n",
    "        LINIEN_ID string,\n",
    "        LINIEN_TEXT string,\n",
    "        UMLAUF_ID string,\n",
    "        VERKEHRSMITTEL_TEXT string,\n",
    "        ZUSATZFAHRT_TF string,\n",
    "        FAELLT_AUS_TF string,\n",
    "        BPUIC string,\n",
    "        HALTESTELLEN_NAME string,\n",
    "        ANKUNFTSZEIT string,\n",
    "        AN_PROGNOSE string,\n",
    "        AN_PROGNOSE_STATUS string,\n",
    "        ABFAHRTSZEIT string,\n",
    "        AB_PROGNOSE string,\n",
    "        AB_PROGNOSE_STATUS string,\n",
    "        DURCHFAHRT_TF string\n",
    "    )\n",
    "    row format delimited fields terminated by ';'\n",
    "    STORED AS ORC\n",
    "    location '/user/{0}/hive/sbb/orc'\n",
    "    TBLPROPERTIES (\"orc.compress\"=\"SNAPPY\")\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**Step 3.** This new table is currently empty. You can import data from another table using the [insert overwrite](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML#LanguageManualDML-InsertingdataintoHiveTablesfromqueries) command. We import  datafrom our `sbb_csv_2020-12` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "insert overwrite table {0}.sbb_orc_2020_12 select * from {0}.sbb_csv_2020_12 \n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the content of the `sbb_orc_2020_12_01` table is similar to the content of the `sbb_csv_2020_01` table. Note that the ordering of the tables can be different. Since we did not impose a particular ordering (order by, sort by), the ordering varies depending on underlying storage formats, and can be arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from {0}.sbb_orc_2020_12 limit 5\n",
    "\"\"\".format(username)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila. You have a created table stored in _ORC_ format. Because the table is external, dropping this table in Hive will not delete the ORC files, and you can reuse them in other Hive tables, or in your Spark applications, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Part 2 - Hive Serialization/Deserialization Format\n",
    "\n",
    "In the next set of exercises we review the methods to used to move external data in and out of Hive tables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.** Import one day of Twitter data into Hive.\n",
    "\n",
    "Create an external table from HDFS dir `/data/twitter/json/2019/09/30` and call it **_username_.**`twitter_2019_09_30`. The table should have a single column named json of type string. Do not forget to use your database for the table instead of the Hive default.\n",
    "\n",
    "A few hints:\n",
    "1. The files have only one field per line\n",
    "2. If you do not specify the row format, the default format fields terminated by '\\n' will be used.\n",
    "\n",
    "After the table `twitter_2019_09_30` is created, select its first row with a select command (limit 1). Use the output of the select query to identify the json fields where the the language and the timestamp information of the tweet are stored. You can use http://jsonprettyprint.com/, or the `jq` command to pretty print the json string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "drop table if exists {0}.twitter_2019_09_30\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "create external table {0}.twitter_2019_09_30(json string)\n",
    "  stored as textfile\n",
    "  location '/data/twitter/json/2019/09/30'\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from {0}.twitter_2019_09_30 limit 1\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Step 2.** Extract JSON fields from raw text format.\n",
    "\n",
    "Hive parses the file as raw text format, ignoring its JSON structure.\n",
    "\n",
    "In the next query we use the following [User Defined Functions](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF) to extract and process the JSON fields from the text.\n",
    "\n",
    "* get_json_object\n",
    "* from_unix_time\n",
    "* cast\n",
    "* min\n",
    "* max\n",
    "\n",
    "Can you guess the meaning of this Hive command?\n",
    "\n",
    "For further reading, you can also learn more about the subtle distinction between [**order by** and **sort by**](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy). It describes the side effects of the underlying _MapReduce_ technology on top of which Hive is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "with q as (\n",
    "    select\n",
    "        get_json_object(json, '$.lang') as lang,\n",
    "            from_unixtime(\n",
    "                cast(\n",
    "                    cast(\n",
    "                        get_json_object(json, '$.timestamp_ms') as bigint\n",
    "                    ) /1000 as bigint\n",
    "                )\n",
    "        ) as time_str\n",
    "    from {0}.twitter_2019_09_30\n",
    ")\n",
    "select lang,count(*) as count,min(time_str) as first_ts_UTC,max(time_str) as last_ts_UTC\n",
    "from q\n",
    "group by lang\n",
    "order by count desc\n",
    "\"\"\".format(username)\n",
    "pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Step 3.** Use available Serialization/Deserialization (_SerDe_) libraries.\n",
    "\n",
    "Using `get_json_object` in every `select` queries can be cubersome, and error prone. Hive provides the [_SerDe_ framework](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-RowFormats&SerDe) to simplify the data IO serialization and deserialization. _SerDe_ properties are specified when Hive tables are created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"drop table if exists {0}.twitter_serde_2019_09_30\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "create external table {0}.twitter_serde_2019_09_30(\n",
    "        timestamp_ms string,\n",
    "        lang string\n",
    "    )\n",
    "    row format serde 'org.apache.hadoop.hive.serde2.JsonSerDe'\n",
    "    WITH SERDEPROPERTIES(\n",
    "        \"ignore.malformed.json\"=\"true\"\n",
    "    )\n",
    "    stored as textfile\n",
    "    location '/data/twitter/json/2019/09/30'\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "    select * from {0}.twitter_serde_2019_09_30 limit 10\n",
    "\"\"\".format(username)\n",
    "pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Part 3 - Hive over HBase\n",
    "\n",
    "Hive is a data warehouse built on top of Hadoop. Hive queries get translated into MapReduce batch jobs that run on a distributed cluster. Hive is thus better for bulk inserts or updates of millions of rows at a time. It is not designed for fast individual lookups, operations on individual rows, or for real-time data. Other databases, such as MongoDB, Cassandra, [Apache Accumulo](https://accumulo.apache.org/) or [HBase](https://hbase.apache.org/) are better suited for real-time data. However they lack the relational DBMS flavor of Hive and an SQL-like interface. They are not ideal for complex relational queries.\n",
    "\n",
    "In the following exercises, we will illustrate how to get the best from both worlds by integrating Hive with HBase. With this configuration it is possible to ingest high rates of individual rows of data, such as sensor measurements, in HBase, and run batch queries, possibly mixed with data HDFS data, using Hive.\n",
    "\n",
    "More details about what follows can be found in the [Hive/HBase integration](https://cwiki.apache.org/confluence/display/Hive/HBaseintegration) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### HBase\n",
    "\n",
    "HBase in a few bullet points:\n",
    "* HBase is a _noSQL_ (or non-relational) database.\n",
    "  - You use HBase when you need random, realtime read/write access to Big Data.\n",
    "  - HBase is schema-less, and is not relational DBMS. We do not use it for relational queries.\n",
    "* It is a _key-value_ store - rows in HBase are indexed by their row keys.\n",
    "* It is a _wide column store_. It can handle billions of rows on millions of columns on clusters of commodity hardware.\n",
    "  - Columns are organized into _column families_.\n",
    "  - _Column families_ must be conjured upfront. They apply to all the rows.\n",
    "  - Columns and their names are not fixed. They are conjured on the fly when rows are created or updated.\n",
    "  - It is a _sparse_ database. Empty columns take no space, they do not exist in HBase.\n",
    "  - Column values are versioned.\n",
    "* Tables have a _name_ and a _namespace_, are are uniquely identified by _namespace:name_\n",
    "* Main data model operations on HBase are:\n",
    "  - `put`: add a new row, or update an existing row\n",
    "  - `get`: get the value from a row\n",
    "  - `scan`: iterate over range of contigous rows, optionally with a _Filter_.\n",
    "  - `delete`: delete a row\n",
    "* It is built on top of HDFS. This is counter intutive, given HDFS's block-based nature. HBase manages it with periodic data compaction.\n",
    "\n",
    "\n",
    "Conceptually, the structure of a HBase table looks like this:\n",
    "\n",
    "| Row key       | timestamp | ColumnFamily1 | ColumnFamily2  |\n",
    "| ------------- |:---------:|:---------------:|:----------------:|\n",
    "| 01.12.2019/80:06____:17004:000 | 1583867978 | produkt_id=Zug,linenid=17004 | bpuic=8500090 |\n",
    "| 01.12.2019/80:06____:17017:000 | 1583868321 | produkt_id=Bus,linenid=701   | bpuic=8301093 |\n",
    "\n",
    "A _{row,column-family:column,version}_ specifes a _cell_ in the table.\n",
    "\n",
    "References: [hbase.apache.org](https://hbase.apache.org/book.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Create a connection to HBase\n",
    "\n",
    "We use [happybase](https://happybase.readthedocs.io/en/latest/) to connect remotely to the HBase server. The happybase API supports a very limited subset of all the commands possible with HBase. For more complex tasks we would use the `hbase` command line interface.\n",
    "\n",
    "The python package is already installed. Otherwise, it can be installed with `pip install happybase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase\n",
    "hbaseaddr = os.environ['HBASE_SERVER']\n",
    "hbase_connection = happybase.Connection(hbaseaddr, transport='framed',protocol='compact')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Create an HBase table\n",
    "\n",
    "We use your ID (from variable username) for the table namespace in HBase, as we did in Hive.\n",
    "\n",
    "You may need to delete the table first if it exists. HBase table must be `disabled` before they can be deleted or altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    hbase_connection.delete_table('{0}:sbb_hbase'.format(username),disable=True)\n",
    "except Exception as e:\n",
    "    print(e.message)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Create a new HBase table, called `sbb_hbase` under your namespace.\n",
    "\n",
    "Note: if we do not create the table, a default table will be created when we create the Hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbase_connection.create_table(\n",
    "    '{0}:sbb_hbase'.format(username),\n",
    "    {'cf1': dict(max_versions=10),\n",
    "     'cf2': dict()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "List all the tables in HBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hbase_connection.tables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Inspect the tables. The properties of the column families can be specified when the table is created, e.g. `{ 'cf1': dict(max_version=10,block_cache_enabled=False) }`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbase_connection.table('{0}:sbb_hbase'.format(username)).families()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "We can scan the HBase table to verify that it is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in hbase_connection.table('{0}:sbb_hbase'.format(username)).scan():\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Create an external Hive table on top of the HBase table. We first delete it if it exists.\n",
    "\n",
    "The table will contain the following fields:\n",
    "* `RowKey`\n",
    "* `BETRIEBSTAG`\n",
    "* `FAHRT_BEZEICHNER`\n",
    "* `ABFAHRTSZEIT`\n",
    "* `BPUIC`\n",
    "\n",
    "RowKey is the key we will use to index the row. Other columns are populated from the sbb table.\n",
    "\n",
    "Note:\n",
    "* The table is stored using the `org.apache.hadoop.hive.hbase.HBaseStorageHandler` for HBase.\n",
    "* TBLPROPERTIES `hbase.table.name` specifies the name of the HBase table that the external Hive table should point to. It is optional, and default to the same name as the Hive table.\n",
    "* SERDEPROPERTIES `hbase.columns.mapping` defines the mapping between the Hive columns and the HBase columns. The definition are listed in the Hive column order, that is `RowKey` maps to `:key`, the key of the HBase table, `BETRIEBSTAG` maps to `cf1:betriebstag` in HBase (Column family cf1, column betriebstag), and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "drop table {0}.sbb_hive_on_hbase\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE EXTERNAL TABLE {0}.sbb_hive_on_hbase(\n",
    "    RowKey string,\n",
    "    BETRIEBSTAG string,\n",
    "    FAHRT_BEZEICHNER string,\n",
    "    ABFAHRTSZEIT string,\n",
    "    BPUIC bigint\n",
    ") \n",
    "STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n",
    "WITH SERDEPROPERTIES (\n",
    "    \"hbase.columns.mapping\"=\":key,cf1:betriebstag,cf1:fahrt_bezeichner,cf2:abfahrtszeit,cf2:bpuic\"\n",
    ")\n",
    "TBLPROPERTIES(\n",
    "    \"hbase.table.name\"=\"{0}:sbb_hbase\",\n",
    "    \"hbase.mapred.output.outputtable\"=\"{0}:sbb_hbase\"\n",
    ")\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "show tables from {0}\n",
    "\"\"\".format(username)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "The external Hive table is backed by the HBase table, which is currently empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from {0}.sbb_hive_on_hbase limit 1\n",
    "\"\"\".format(username)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "We may now populate the HBase table with SBB data. We do this through a `insert overwrite ... select`, as before. We copy from {0}.sbb_orc, to {0}.sbb_hive_on_hbase. This command may take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "insert overwrite table {0}.sbb_hive_on_hbase\n",
    "    select\n",
    "         concat(BETRIEBSTAG,\":\",FAHRT_BEZEICHNER) as RowKey,\n",
    "         BETRIEBSTAG,\n",
    "         FAHRT_BEZEICHNER,\n",
    "         ABFAHRTSZEIT,\n",
    "         BPUIC\n",
    "    from {0}.sbb_orc_2020_12 limit 20\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "A scan the HBase table shows the rows inserted by Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in hbase_connection.table('{0}:sbb_hbase'.format(username)).scan():\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Cleanup (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "drop table {0}.sbb_hive_on_hbase\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "drop table {0}.sbb_orc_2020_12\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "drop table {0}.sbb_csv_2020_12 \n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    hbase_connection.delete_table('{0}:sbb_hbase'.format(username),disable=True)\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources:\n",
    "* Hive toturial: [https://cwiki.apache.org/confluence/display/Hive/Tutorial](https://cwiki.apache.org/confluence/display/Hive/Tutorial)\n",
    "* HBase integration: [https://cwiki.apache.org/confluence/display/Hive/HBaseintegration](https://cwiki.apache.org/confluence/display/Hive/HBaseintegration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
